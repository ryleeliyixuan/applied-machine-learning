{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c9e64aa",
   "metadata": {},
   "source": [
    "# HW 2 - PROGRAMMING EXERCISES - Binary Classification on Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13a1b2f",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a17dc097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import string\n",
    "import seaborn as sns\n",
    "# import nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import word_tokenize, pos_tag\n",
    "# import sklearn\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88a1b36",
   "metadata": {},
   "source": [
    "## (a) Download the data\n",
    "I downloaded the data set and successfully imported it by implementing following code:\n",
    "\n",
    "(1) how many training and test data points are there? \n",
    "\n",
    "7613 in the trainning set and 3263 in the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d94cdfdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load datasets\n",
    "df_test  = pd.read_csv(\"test.csv\");\n",
    "df_train = pd.read_csv(\"train.csv\");\n",
    "\n",
    "# check data\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "536628b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data\n",
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "197f34e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613 3263\n"
     ]
    }
   ],
   "source": [
    "# basic insepction \n",
    "print(df_train.shape[0], df_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92595cb0",
   "metadata": {},
   "source": [
    "(2) what percentage of the training tweets are of real disasters, and what percentage is not?\n",
    "\n",
    "42.97% are of real disasters, 57.03% are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8924dc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of the training tweets are of real disasters: 42.96597924602653%.\n"
     ]
    }
   ],
   "source": [
    "# percentage of the trainning tweets are of real disasters\n",
    "df_train.loc[df_train['target'] == 1].shape\n",
    "percentage_real = df_train.loc[df_train['target'] == 1].shape[0] / df_train.shape[0]\n",
    "print(\"Percentage of the training tweets are of real disasters: \" + str(percentage_real * 100) + \"%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1f517d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of the training tweets are NOT of real disasters: 57.03402075397347%.\n"
     ]
    }
   ],
   "source": [
    "# percentage of the trainning tweets are of not real disasters\n",
    "percentage_fake = df_train.loc[df_train['target'] == 0].shape[0] / df_train.shape[0]\n",
    "print(\"Percentage of the training tweets are NOT of real disasters: \" + str(percentage_fake * 100) + \"%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf63cd9",
   "metadata": {},
   "source": [
    "## (b) Split the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2eed4824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5329, 5), (2284, 5))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly choose 70% of the data points in the training data as the training set\n",
    "# and the remaining 30% of the data as the development set\n",
    "train, dev = train_test_split(df_train, test_size=0.3)\n",
    "train.shape, dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3520d57",
   "metadata": {},
   "source": [
    "## (c) Preprocess the data\n",
    "The data sets contain significant amounts of noise and unprocessed content. Data cleaning and pre-processing methods would be applied. I explained the reasons for each of my decision (why or why not) in the following paragraph: \n",
    "* Convert all the words to lowercase: Yes. I think whether the word is lowercase or uppercase doesn't contribute to the classification in the context of our problem. And users (especially Twitter users) may not strictly follow the grammar and use lowercase and uppercase arbitrarily. In this sense, building the vector based on cases is meaningless and makes our operations inefficient. I chose to convert all the words to lowercase. \n",
    "* Lemmatize all the words: Yes. I think the tense of words doesn't contribute much to the classification in the context of our problem. So I lemmatized all the words based on their POS tags using WordNetLemmatizer from the nltk library. I didn't use nltk.stem since stemming may create the non-existence meaning of a word (eg. from \"causes\" to \"caus\"). But lemmatization always gives the dictionary meaning word while converting into root-form. \n",
    "* Strip punctuation: Yes. I think punctuation barely contributes to our analysis of text. Users' choices of punctuation don't impact the meaning of their tweets. So I stripped punctuation using the Python built-in library. \n",
    "* Strip the stop words, e.g., “the”, “and”, “or”: Yes. Stop words don't not add much information to the text and should be filtered out.\n",
    "* Strip @ and urls. (It’s Twitter.): Yes for ulrs, and No for @. I stripped urls since they are references to a location on the web, but do not provide any additional information. It's meaningless to do NLP and get conclusion from these non-semantic urls. And urls are lengthy, thus causing inefficiency while getting bag of words. I use the re library that provides regular expression matching operations to strip them. I chose not to strip @ since mention of certain users may contribute the our classification problem. For instance, it's possible that mentioning @earthquakeBot may be a feature that contribute to tweets that are real disasters since the followers of this account closely follow this topic (this is just an assumption). \n",
    "* Something else: Strip the HTML tags - I found some HTML tags like &amp in the text. They do not add any value to text data and only enable proper browser rendering. So I removed them using re library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e8c6da9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ryleeli/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /Users/ryleeli/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ryleeli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import packages from nltk and re\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re # Regex library\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get pos tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "# Lemmatize all words\n",
    "def Lemmatize_Sentence(sentence):\n",
    "    res = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for word, pos in pos_tag(word_tokenize(sentence)):\n",
    "        wordnet_pos = get_wordnet_pos(pos) or wordnet.NOUN\n",
    "        res.append(lemmatizer.lemmatize(word, pos=wordnet_pos))\n",
    "    return \" \".join(res)\n",
    "\n",
    "# Strip Punctuation\n",
    "def Strip_Punct(text):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "# Strip Stop words\n",
    "stopword = stopwords.words('english')\n",
    "def Strip_StopWord(string):\n",
    "    string_list = string.split()\n",
    "    return ' '.join(i for i in string_list if i not in stopword)\n",
    "\n",
    "# Strip Urls\n",
    "def Strip_Url(string):\n",
    "    return re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%|\\-)*\\b', '', string)\n",
    "\n",
    "#  Strip HTML Tags\n",
    "def Strip_HTML(text):\n",
    "    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    return re.sub(html, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f7deeb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all the words to lowercase, then apply all the above processing methods\n",
    "train['text'] = train['text'].str.lower().apply(Lemmatize_Sentence).apply(Strip_Punct).apply(Strip_StopWord).apply(Strip_Url).apply(Strip_HTML)\n",
    "dev['text'] = dev['text'].str.lower().apply(Lemmatize_Sentence).apply(Strip_Punct).apply(Strip_StopWord).apply(Strip_Url).apply(Strip_HTML)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6a6d35",
   "metadata": {},
   "source": [
    "## (d) Bag of words model\n",
    "I chose threshold $M = 3$ to avoid run-time and memory issues, and to avoid noisy/unreliable features that can hurt learning. Intuitively, I was thinking about $M\\in[2, 10]$. If $M$ was too small, a run-time and memory issues may occur since we may have very high-dimensional vectors. And too many features may cause over-fitting and bad generalizes. If $M$ was too large, fewer words/features were selected, thus leading to a poor-performed model that cannot generalize well to new data. Then I ran a test for $M\\in[2, 10]$ with the implementation of regularized logistic regression, and I decided to choose $M = 3$ as it reflects the highest F1-score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "281680a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features in these vectors: 2917\n"
     ]
    }
   ],
   "source": [
    "# Choose threshold to avoid run-time and memory issues, and to avoid noisy/unreliable features that can hurt learning\n",
    "threshold = 3\n",
    "# Build the bag of words feature vectors\n",
    "def cv(data):\n",
    "    count_vectorizer = CountVectorizer(min_df = threshold, binary=True) \n",
    "    emb = count_vectorizer.fit_transform(data)\n",
    "    return emb, count_vectorizer\n",
    "# Tokenizing the pre-processed clean texts\n",
    "train['tokenized'] = train[\"text\"].tolist()\n",
    "dev['tokenized'] = dev[\"text\"].tolist()\n",
    "# Build the bag of words feature vectors for both the training and development sets\n",
    "train_bag_of_words, train_cv = cv(train['tokenized'])\n",
    "dev_bag_of_words = train_cv.transform(dev['tokenized'])\n",
    "train_words = train_cv.get_feature_names()\n",
    "# Report the total number of features in these vectors\n",
    "print(\"Total number of features in these vectors: \"+ str(len(train_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1719541",
   "metadata": {},
   "source": [
    "## (e) Logistic regression\n",
    "### (e.i) Logistic regression without regularization terms\n",
    "I observed issues with overfitting. This is a very expressive model that fits the training dataset perfectly but makes highly incorrect predictions outside this dataset, and doesn't generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3911913f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression without regularization terms - F1 Score on training set: 0.9804177545691906\n",
      "Logistic regression without regularization terms - F1 Score on development set: 0.68407835258664\n"
     ]
    }
   ],
   "source": [
    "# Train a logistic regression model without regularization terms\n",
    "LR0_Model = LogisticRegression(penalty='none')\n",
    "LR0_Model.fit(train_bag_of_words, train['target'])\n",
    "train_predicted_LR0 = LR0_Model.predict(train_bag_of_words)\n",
    "dev_predicted_LR0 = LR0_Model.predict(dev_bag_of_words)\n",
    "#  Report the F1 score in the training and in the development set\n",
    "print(\"Logistic regression without regularization terms - F1 Score on training set: \" + str(f1_score(train_predicted_LR0, train['target'])))\n",
    "print(\"Logistic regression without regularization terms - F1 Score on development set: \" + str(f1_score(dev_predicted_LR0, dev['target'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a49740",
   "metadata": {},
   "source": [
    "### (e.ii) Logistic regression with L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "74786612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression with L1 regularization - F1 Score on training set: 0.8439814814814816\n",
      "Logistic regression with L1 regularization - F1 Score on development set: 0.7417366946778711\n"
     ]
    }
   ],
   "source": [
    "# Train a logistic regression model with L1 regularization\n",
    "LR_L1_Model = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "LR_L1_Model.fit(train_bag_of_words, train['target'])\n",
    "train_predicted_LR_L1 = LR_L1_Model.predict(train_bag_of_words)\n",
    "dev_predicted_LR_L1 = LR_L1_Model.predict(dev_bag_of_words)\n",
    "#  Report the F1 score in the training and in the development set\n",
    "print(\"Logistic regression with L1 regularization - F1 Score on training set: \" + str(f1_score(train_predicted_LR_L1, train['target'])))\n",
    "print(\"Logistic regression with L1 regularization - F1 Score on development set: \" + str(f1_score(dev_predicted_LR_L1, dev['target'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c027d0",
   "metadata": {},
   "source": [
    "### (e.iii) Logistic regression with L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7420f780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression with L2 regularization - F1 Score on training set: 0.8846855059252506\n",
      "Logistic regression with L2 regularization - F1 Score on development set: 0.7472283813747228\n"
     ]
    }
   ],
   "source": [
    "# Train a logistic regression model with L2 regularization\n",
    "LR_L2_Model = LogisticRegression(penalty='l2', solver='liblinear')\n",
    "LR_L2_Model.fit(train_bag_of_words, train['target'])\n",
    "train_predicted_LR_L2 = LR_L2_Model.predict(train_bag_of_words)\n",
    "dev_predicted_LR_L2 = LR_L2_Model.predict(dev_bag_of_words)\n",
    "#  Report the F1 score in the training and in the development set\n",
    "print(\"Logistic regression with L2 regularization - F1 Score on training set: \" + str(f1_score(train_predicted_LR_L2, train['target'])))\n",
    "print(\"Logistic regression with L2 regularization - F1 Score on development set: \" + str(f1_score(dev_predicted_LR_L2, dev['target'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7d2d3c",
   "metadata": {},
   "source": [
    "### (e.iv) Which one of the three classifiers performed the best on your training and development set? Did you observe any overfitting and did regularization help reduce it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8b507b",
   "metadata": {},
   "source": [
    "The logistic regression without regularization performed the best on our development set. The logistic regression with L2 regularization performed the best on our development set. \n",
    "* Logistic regression model with L2 regularization performed the best on the training and development set with a F1-score of 0.8846 on the training set and a F1-score of 0.7472 on the development set. This model reflects the highest F1-score on the development set and reduces the problem of over-fitting with regularization. \n",
    "* Yes, I observed issues of over-fitting while implementing logistic regression model without regularization terms. The model reflects a F1-score of 0.9804 on the training set and a F1-score of 0.6841 on the development set. It fits the training dataset perfectly with a high F1-score but performs poor predictions outside the training set (ie. the development set).\n",
    "* Both L1 regularization and L2 regularization help reduce the over-fitting. After regularization, the F1-score on development set increases from 0.6841 to 0.7417, 0.7472 respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b93c471",
   "metadata": {},
   "source": [
    "### (e.v) Inspect the weight vector of the classifier with L1 regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0d5e9e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           words      coef\n",
      "2695     typhoon  3.562875\n",
      "2404       spill  3.343851\n",
      "2830    wildfire  3.187095\n",
      "837   earthquake  3.179425\n",
      "731   derailment  3.154386\n",
      "1234   hiroshima  3.083647\n",
      "1625     migrant  2.722774\n",
      "2254     selfies  2.699474\n",
      "1571    massacre  2.692738\n",
      "2725      usagov  2.541668\n",
      "1616       mh370  2.455677\n",
      "759          dig  2.446691\n",
      "843        ebola  2.438449\n",
      "2767     volcano  2.364489\n",
      "1834    outbreak  2.352875\n",
      "2325    sinkhole  2.302689\n",
      "1701    murderer  2.226004\n",
      "697       debris  2.219851\n",
      "2415        stab  2.204481\n",
      "645         crew  2.183769\n"
     ]
    }
   ],
   "source": [
    "# Inspect the weight vector of the classifier with L1 regularization \n",
    "coefficients = pd.concat([pd.DataFrame(train_words, columns=['words']),\n",
    "                          pd.DataFrame(np.transpose(np.abs(LR_L1_Model.coef_)), columns=['coef'])], axis=1)\n",
    "# Sort the coefficients\n",
    "coefficients = coefficients.sort_values(by=['coef'], ascending=False)\n",
    "# List the most important words for deciding whether a tweet is about areal disaster or not\n",
    "print(coefficients.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694b7392",
   "metadata": {},
   "source": [
    "## (f) Bernoulli Naive Bayes\n",
    "I implemented a Bernoulli Naive Bayes classifier with additive smoothing to predict the probability of whether each tweet is about a real disaster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "96d007f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.56671045 0.43328955]\n"
     ]
    }
   ],
   "source": [
    "# Compute the maximum likelihood model parameters on our dataset\n",
    "n = train_bag_of_words.shape[0] # size of the dataset\n",
    "d = train_bag_of_words.shape[1] # number of features in our dataset\n",
    "K = 2 # number of clases\n",
    "\n",
    "# # these are the shapes of the parameters\n",
    "psis = np.zeros([K,d])\n",
    "phis = np.zeros([K])\n",
    "alpha = 1 # additive smoothing \n",
    "\n",
    "# we now compute the parameters\n",
    "for k in range(K):\n",
    "    X_k = train_bag_of_words[train['target'] == k]\n",
    "    # psis[k] = np.mean(X_k, axis=0)\n",
    "    psis[k] = (X_k.sum(axis=0) + alpha) / (2 * alpha + X_k.shape[0])\n",
    "    phis[k] = (X_k.shape[0]) / (float(n))\n",
    "\n",
    "# print out the class proportions\n",
    "print(phis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "160da81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predictions using Bayes' rule and implement the model in numpy\n",
    "def nb_predictions(x, psis, phis):\n",
    "    \"\"\"This returns class assignments and scores under the NB model.\n",
    "    We compute \\arg\\max_y p(y|x) as \\arg\\max_y p(x|y)p(y)\n",
    "    \"\"\"\n",
    "    # adjust shapes\n",
    "    n, d = x.shape\n",
    "    x = np.reshape(x, (1, n, d))\n",
    "    psis = np.reshape(psis, (K, 1, d))\n",
    "    \n",
    "    # clip probabilities to avoid log(0)\n",
    "    psis = psis.clip(1e-14, 1-1e-14)\n",
    "    \n",
    "    # compute log-probabilities\n",
    "    logpy = np.log(phis).reshape([K,1])\n",
    "    logpxy = x * np.log(psis) + (1-x) * np.log(1-psis)\n",
    "    logpyx = logpxy.sum(axis=2) + logpy\n",
    "\n",
    "    return logpyx.argmax(axis=0).flatten(), logpyx.reshape([K,n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "76729908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes - F1 Score on training set: 0.8105163429654192\n",
      "Bernoulli Naive Bayes - F1 Score on development set: 0.7416378316032295\n"
     ]
    }
   ],
   "source": [
    "# Train this classifier on the training set\n",
    "idx_train, logpyx_train = nb_predictions(train_bag_of_words.toarray(), psis, phis)\n",
    "idx_dev, logpyx_dev = nb_predictions(dev_bag_of_words.toarray(), psis, phis)\n",
    "# Report its F1-score on the development set\n",
    "print(\"Bernoulli Naive Bayes - F1 Score on training set: \" + str(f1_score(idx_train, train['target'])))\n",
    "print(\"Bernoulli Naive Bayes - F1 Score on development set: \" + str(f1_score(idx_dev, dev['target'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6ff719",
   "metadata": {},
   "source": [
    "## (g) Model Comparsion\n",
    "Check Report for this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56791b1a",
   "metadata": {},
   "source": [
    "## (h) N-gram model\n",
    "I chose threshold $M = 6$. If $M$ was too small, a run-time and memory issues may occur since we may have very high-dimensional vectors, especially when building a n-gram model. And too many features may cause over-fitting and bad generalizations. If $M$ was too large, fewer words/features (especially bi-grams) were selected, thus leading to a poor-performed model that cannot generalize well to new data. Then I ran a test for $M\\in[2, 10]$. I decided to choose $M = 6$ as it obtains 2112 words in the vocabulary (1-grams and 2-grams), which is closed to the total number of words in the vocabulary when we built the bag of words for previous models (ie. 2917). As we built bags of words with similar size, the comparison between models would be more insightful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "b5f108cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose threshold to avoid run-time and memory issues\n",
    "# and to avoid noisy/unreliable features that can hurt learning\n",
    "threshold = 6\n",
    "# Build the bag of words feature vectors\n",
    "def ngram_cv(data):\n",
    "    count_vectorizer = CountVectorizer(min_df = threshold, binary=True, ngram_range=(1,2)) \n",
    "    emb = count_vectorizer.fit_transform(data)\n",
    "    return emb, count_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "5365461a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of 1-grams and 2-grams in the vocabulary: 2112\n",
      "The total number of 1-gram is: 1570\n",
      "The total number of 2-grams is: 542\n"
     ]
    }
   ],
   "source": [
    "# Build the bag of words feature vectors for both the training and development sets\n",
    "train_ngram_bag_of_words, train_ngram_cv = ngram_cv(train['tokenized'])\n",
    "dev_ngram_bag_of_words = train_ngram_cv.transform(dev['tokenized'])\n",
    "train_ngram_words = train_ngram_cv.get_feature_names()\n",
    "# Report the total number of 1-grams and 2-grams in the vocabulary.\n",
    "print(\"Total number of 1-grams and 2-grams in the vocabulary: \"+ str(train_ngram_bag_of_words.shape[1]))\n",
    "\n",
    "# count 1-gram and 2-gram respectively \n",
    "one_gram = 0 \n",
    "two_gram = 0 \n",
    "\n",
    "for word in train_ngram_words:\n",
    "    if ' ' in word:\n",
    "        two_gram += 1\n",
    "    else:\n",
    "        one_gram += 1 \n",
    "# print out the result       \n",
    "print('The total number of 1-gram is:', one_gram) \n",
    "print('The total number of 2-grams is:', two_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "b5f6261b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11yearold boy', '12000 nigerian', '15 saudi', '16yr old', '1980 http', '2015 http', '3g whole', '40 family', '5km volcano', '70 year']\n"
     ]
    }
   ],
   "source": [
    "# Take 10 2-grams from your vocabulary, and print them out\n",
    "k = 0\n",
    "i = 0\n",
    "bigram = [0 for i in range(10)] \n",
    "while k < 10 and i < len(train_ngram_words):\n",
    "    words = train_ngram_words[i]\n",
    "    wordslist = words.split(\" \")\n",
    "    if len(wordslist) == 2:\n",
    "        bigram[k] = words\n",
    "        k += 1\n",
    "    i += 1\n",
    "print(bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fb4870",
   "metadata": {},
   "source": [
    "### Logistic Regression with L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "7533a08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression with L2 regularization - N gram - F1 Score on training set: 0.8554382744378155\n",
      "Logistic regression with L2 regularization - N gram - F1 Score on development set: 0.741111111111111\n"
     ]
    }
   ],
   "source": [
    "# Train a logistic regression model with L2 regularization\n",
    "LR_L2_ngram_Model = LogisticRegression(penalty='l2', solver='liblinear')\n",
    "LR_L2_ngram_Model.fit(train_ngram_bag_of_words, train['target'])\n",
    "train_predicted_LR_L2_ngram = LR_L2_ngram_Model.predict(train_ngram_bag_of_words)\n",
    "dev_predicted_LR_L2_ngram = LR_L2_ngram_Model.predict(dev_ngram_bag_of_words)\n",
    "#  Report the F1 score in the training and in the development set\n",
    "print(\"Logistic regression with L2 regularization - N gram - F1 Score on training set: \" \n",
    "          + str(f1_score(train_predicted_LR_L2_ngram, train['target'])))\n",
    "print(\"Logistic regression with L2 regularization - N gram - F1 Score on development set: \" \n",
    "          + str(f1_score(dev_predicted_LR_L2_ngram, dev['target'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c28a812",
   "metadata": {},
   "source": [
    "### Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "ae34e80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.56671045 0.43328955]\n"
     ]
    }
   ],
   "source": [
    "# Compute the maximum likelihood model parameters on our dataset\n",
    "n = train_ngram_bag_of_words.shape[0] # size of the dataset\n",
    "d = train_ngram_bag_of_words.shape[1] # number of features in our dataset\n",
    "K = 2 # number of clases\n",
    "\n",
    "# these are the shapes of the parameters\n",
    "psis = np.zeros([K,d])\n",
    "phis = np.zeros([K])\n",
    "alpha = 1 # additive smoothing \n",
    "\n",
    "# we now compute the parameters\n",
    "for k in range(K):\n",
    "    X_k = train_ngram_bag_of_words[train['target'] == k]\n",
    "    # psis[k] = np.mean(X_k, axis=0)\n",
    "    psis[k] = (X_k.sum(axis=0) + alpha) / (2 * alpha + X_k.shape[0])\n",
    "    phis[k] = (X_k.shape[0]) / (float(n))\n",
    "\n",
    "# print out the class proportions\n",
    "print(phis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "3f4a408e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes - N gram - F1 Score on training set: 0.7544574630667344\n",
      "Bernoulli Naive Bayes - N gram - F1 Score on development set: 0.7045596502186133\n"
     ]
    }
   ],
   "source": [
    "# Train this classifier on the training set\n",
    "idx_train_ngram, logpyx_train_ngram = nb_predictions(train_ngram_bag_of_words.toarray(), psis, phis)\n",
    "idx_dev_ngram, logpyx_dev_ngram = nb_predictions(dev_ngram_bag_of_words.toarray(), psis, phis)\n",
    "# Report its F1-score on the development set\n",
    "print(\"Bernoulli Naive Bayes - N gram - F1 Score on training set: \" \n",
    "          + str(f1_score(idx_train_ngram, train['target'])))\n",
    "print(\"Bernoulli Naive Bayes - N gram - F1 Score on development set: \" \n",
    "          + str(f1_score(idx_dev_ngram, dev['target'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9bd7ca",
   "metadata": {},
   "source": [
    "## (i) Determine performance with the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "fa56fb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process data\n",
    "df_train[\"text\"] = df_train[\"text\"].str.lower().apply(Lemmatize_Sentence).apply(Strip_Punct).apply(Strip_StopWord).apply(Strip_Url).apply(Strip_HTML)\n",
    "df_test[\"text\"] = df_test[\"text\"].str.lower().apply(Lemmatize_Sentence).apply(Strip_Punct).apply(Strip_StopWord).apply(Strip_Url).apply(Strip_HTML)\n",
    "# Tokenizing the pre-processed clean texts\n",
    "df_train['tokenized'] = df_train[\"text\"].tolist()\n",
    "df_test['tokenized'] = df_test[\"text\"].tolist()\n",
    "# Re-build the feature vectors\n",
    "train_BOW, data_cv = cv(df_train['tokenized'])\n",
    "test_BOW = data_cv.transform(df_test['tokenized'])\n",
    "# Re-train the preferred classifier - Logistic Regression with L2 regularization\n",
    "LR_L2_Model = LogisticRegression(penalty='l2', solver='liblinear')\n",
    "LR_L2_Model.fit(train_BOW, df_train['target'])\n",
    "train_predicted = LR_L2_Model.predict(train_BOW)\n",
    "test_predicted = LR_L2_Model.predict(test_BOW)\n",
    "# save predictions\n",
    "res=pd.DataFrame(columns = ['id', 'target'])\n",
    "res['id'] = df_test['id']\n",
    "res['target'] = test_predicted\n",
    "res.to_csv('my_submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
