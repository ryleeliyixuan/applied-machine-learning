{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7808aaf8",
   "metadata": {},
   "source": [
    "# Lecture 1: Introduction to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a889e574",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supervised Learning\n",
    "\n",
    "The most common approach to machine learning is supervised learning.\n",
    "\n",
    "<center><img width=70% src=\"img/tesla_data.png\"/></center>\n",
    "\n",
    "1. First, we collect a dataset of labeled training examples.\n",
    "2. We train a model to output accurate predictions on this dataset.\n",
    "3. When the model sees new, similar data, it will also be accurate.\n",
    "\n",
    "We can use this dataset of examples to fit a supervised learning model.\n",
    "\n",
    "* The model maps input $x$ (the education level) to output a $y$ (the house price).\n",
    "* It learns the mapping from our dataset of examples $(x, y)$.\n",
    "\n",
    "Why?\n",
    "* Making predictions on new data.\n",
    "* Understanding the mechanisms through which input variables affect targets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746820cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "Here, we have a dataset *without* labels. Our goal is to learn something interesting about the structure of the data:\n",
    "* Clusters hidden in the dataset.\n",
    "* Outliers: particularly unusual and/or interesting datapoints.\n",
    "* Useful signal hidden in noise, e.g. human speech over a noisy phone.\n",
    "\n",
    "We can use this dataset of examples to fit an unsupervised learning model.\n",
    "* **The model defines a probability distribution over the inputs.**\n",
    "* The probability distribution identifies multiple components (multiple peaks).\n",
    "* The components indicate structure in the data.\n",
    "\n",
    "Unsupervised learning also has numerous applications:\n",
    "* Recommendation systems: suggesting movies on Netflix.\n",
    "* Anomaly detection: identifying factory components that are likely to break soon.\n",
    "* Signal denoising: extracting human speech from a noisy recording."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59060f7d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "In reinforcement learning, an agent is interacting with the world over time. We teach it good behavior by providing it with rewards.\n",
    "\n",
    "<center><img src=\"img/rl.png\"/></center>\n",
    "\n",
    "<sub><sup>Image by Lily Weng</sup></sub>\n",
    "\n",
    "Applications of reinforcement learning include:\n",
    "* Creating agents that play games such as Chess or Go.\n",
    "* Controling the cooling systems of datacenters to use energy more efficiently.\n",
    "* Designing new drug compounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e114c",
   "metadata": {},
   "source": [
    "# Lecture 2: Supervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146d98f1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary: Components of a Supervised Machine Learning Problem\n",
    "\n",
    "To apply supervised learning, we define a dataset and a learning algorithm.\n",
    "\n",
    "$$ \\underbrace{\\text{Dataset}}_\\text{Features, Attributes, Targets} + \\underbrace{\\text{Learning Algorithm}}_\\text{Model Class + Objective + Optimizer } \\to \\text{Predictive Model} $$\n",
    "\n",
    "The output is a predictive model that maps inputs to targets. For instance, it can predict targets on new inputs.\n",
    "\n",
    "We can also define the high-level structure of a supervised learning algorithm as consisting of three components:\n",
    "* A __model class__: the set of possible models we consider.\n",
    "* An __objective__ function, which defines how good a model is.\n",
    "* An __optimizer__, which finds the best predictive model in the model class according to the objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5300b173",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning: BMI and Diabate Risk Case\n",
    "## Model Class\n",
    "Formally, the model class is a set \n",
    "$$\\mathcal{M} \\subseteq \\{f \\mid f : \\mathcal{X} \\to \\mathcal{Y} \\}$$\n",
    "of possible models that map input features to targets.\n",
    "## Optimizer\n",
    "Given our assumption that $x,y$ follow the a linear relationship, the goal of a supervised learning algorithm is to find a good set of parameters consistent with the data.\n",
    "## Predictive Model\n",
    "The supervised learning algorithm gave us a pair of parameters $\\theta_1^*, \\theta_0^*$. These define the *predictive model* $f^*$, defined as\n",
    "$$ f(x) = \\theta_1^* \\cdot x + \\theta_0^*, $$\n",
    "where again $x$ is the BMI, and $y$ is the diabetes risk score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1767b74",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training Dataset\n",
    "## Inputs\n",
    "\n",
    "More precisely, an input $x^{(i)} \\in \\mathcal{X}$ is a $d$-dimensional vector of the form\n",
    "$$ x^{(i)} = \\begin{bmatrix}\n",
    "x^{(i)}_1 \\\\\n",
    "x^{(i)}_2 \\\\\n",
    "\\vdots \\\\\n",
    "x^{(i)}_d\n",
    "\\end{bmatrix}$$\n",
    "For example, it could be the values of the $d$ features for patient $i$.\n",
    "\n",
    "The set $\\mathcal{X}$ is called the feature space. Often, we have, $\\mathcal{X} = \\mathbb{R}^d$.\n",
    "\n",
    "## Attributes\n",
    "\n",
    "We refer to the numerical variables describing the patient as *attributes*. Examples of attributes include:\n",
    "* The age of a patient.\n",
    "* The patient's gender.\n",
    "* The patient's BMI.\n",
    "\n",
    "## Features\n",
    "Often, an input object has many attributes, and we want to **use these attributes to define more complex descriptions of the input.**\n",
    "\n",
    "* Is the patient old and a man? (Useful if old men are at risk).\n",
    "* Is the BMI above the obesity threshold?\n",
    "\n",
    "We call these custom attributes *features*.\n",
    "\n",
    "Oldman feature:\n",
    "diabetes_X['old_man'] = (diabetes_X['sex'] > 0) & (diabetes_X['age'] > 0.05)\n",
    "\n",
    "We may denote features via a function $\\phi : \\mathcal{X} \\to \\mathbb{R}^p$ that takes an input $x^{(i)} \\in \\mathcal{X}$ and outputs a $p$-dimensional vector\n",
    "$$ \\phi(x^{(i)}) = \\left[\\begin{array}{@{}c@{}}\n",
    "\\phi(x^{(i)})_1 \\\\\n",
    "\\phi(x^{(i)})_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\phi(x^{(i)})_p\n",
    "\\end{array} \\right]$$\n",
    "We say that $\\phi(x^{(i)})$ is a *featurized* input, and each $\\phi(x^{(i)})_j$ is a *feature*.\n",
    "\n",
    "### Features: Discrete vs. Continuous \n",
    "\n",
    "Features can be either discrete or continuous. We will see that some ML algorthims handle these differently.\n",
    "\n",
    "## Targets\n",
    "\n",
    "For each patient, we are interested in predicting a quantity of interest, the *target*. In our example, this is the patient's diabetes risk.\n",
    "\n",
    "Formally, when $(x^{(i)}, y^{(i)})$ form a *training example*, each $y^{(i)} \\in \\mathcal{Y}$ is a target. We call $\\mathcal{Y}$ the target space.\n",
    "\n",
    "### Targets: Regression vs. Classification\n",
    "\n",
    "We distinguish between two broad types of supervised learning problems that differ in the form of the target variable.\n",
    "\n",
    "1. __Regression__: The target variable $y$ is continuous. We are fitting a curve in a high-dimensional feature space that approximates the shape of the dataset.\n",
    "2. __Classification__: The target variable $y$ is discrete. Each discrete value corresponds to a *class* and we are looking for a hyperplane that separates the different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ae12eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model Class: Notation\n",
    "\n",
    "Formally, the model class is a set \n",
    "$$\\mathcal{M} \\subseteq \\{f \\mid f : \\mathcal{X} \\to \\mathcal{Y} \\}$$\n",
    "of possible models that map input features to targets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9d6ff5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When the models $f_\\theta$ are paremetrized by *parameters* $\\theta \\in \\Theta$ living in some set $\\Theta$. Thus we can also write\n",
    "$$\\mathcal{M} = \\{f_\\theta \\mid \\theta \\in \\Theta \\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc91176",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model Class: Example\n",
    "\n",
    "One simple approach is to assume that $x$ and $y$ are related by a linear model of the form\n",
    "\\begin{align*}\n",
    "y & = \\theta_0 + \\theta_1 \\cdot x_1 + \\theta_2 \\cdot x_2 + ... + \\theta_d \\cdot x_d\n",
    "\\end{align*}\n",
    "where $x$ is a featurized input and $y$ is the target.\n",
    "\n",
    "The $\\theta_j$ are the *parameters* of the model, $\\Theta = \\mathbb{R}^{d+1}$, and $\\mathcal{M} = \\{ \\theta_0 + \\theta_1 \\cdot x_1 + \\theta_2 \\cdot x_2 + ... + \\theta_d \\cdot x_d \\mid \\theta \\in \\mathbb{R}^{d+1} \\}$\n",
    "<!-- By using the notation $x_0 = 1$, we can represent the model in a vectorized form\n",
    "$$ y = \\sum_{j=0}^d \\beta_j \\cdot x_j = \\vec \\beta \\cdot \\vec x. $$\n",
    "where $\\vec x$ is a vector of features. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a706dbe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Objectives: Notation\n",
    "\n",
    "<!-- Given a training set, how do we pick the parameters $\\theta$ for the model? A natural approach is to select $\\theta$ such that $f_\\theta(x^{(i)})$ is close to $y^{(i)}$ on a training dataset $\\mathcal{D} = \\{(x^{(i)}, y^{(i)}) \\mid i = 1,2,...,n\\}$. -->\n",
    "\n",
    "To capture this intuition, we define an *objective function* (also called a *loss function*)\n",
    "$$J(f) : \\mathcal{M} \\to [0, \\infty), $$\n",
    "which describes the extent to which $f$ \"fits\" the data $\\mathcal{D} = \\{(x^{(i)}, y^{(i)}) \\mid i = 1,2,...,n\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a8a52e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When $f$ is parametrized by $\\theta \\in \\Theta$, the objective becomes a function $J(\\theta) : \\Theta \\to [0, \\infty).$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780f3115",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Objective: Examples\n",
    "\n",
    "What would are some possible objective functions? We will see many, but here are a few examples:\n",
    "* Mean squared error: $$J(\\theta) = \\frac{1}{2n} \\sum_{i=1}^n \\left( f_\\theta(x^{(i)}) - y^{(i)} \\right)^2$$\n",
    "* Absolute (L1) error: $$J(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\left| f_\\theta(x^{(i)}) - y^{(i)} \\right|$$\n",
    "\n",
    "These are defined for a dataset $\\mathcal{D} = \\{(x^{(i)}, y^{(i)}) \\mid i = 1,2,...,n\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8038c73",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimizer: Notation\n",
    "\n",
    "At a high-level an optimizer takes an objective $J$ and a model class $\\mathcal{M}$ and finds a model $f \\in \\mathcal{M}$ with the smallest value of the objective $J$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{f \\in \\mathcal{M}} J(f)\n",
    "\\end{align*}\n",
    "\n",
    "Intuitively, this is the function that bests \"fits\" the data on the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378b6828",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When $f$ is parametrized by $\\theta \\in \\Theta$, the optimizer minimizes a function $J(\\theta)$ over all $\\theta \\in \\Theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfef57f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimizer: Example\n",
    "\n",
    "We will see that behind the scenes, the `sklearn.linear_models.LinearRegression` algorithm optimizes the MSE loss.\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{\\theta \\in \\mathbb{R}} \\frac{1}{2n} \\sum_{i=1}^n \\left( f_\\theta(x^{(i)}) - y^{(i)} \\right)^2\n",
    "\\end{align*}\n",
    "\n",
    "We can easily measure the quality of the fit on the training set and the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908cba0a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Notation: Feature Matrix\n",
    "\n",
    "Suppose that we have a dataset of size $n$ (e.g., $n$ patients), indexed by $i=1,2,...,n$. Each $x^{(i)}$ is a vector of $d$ features.\n",
    "\n",
    "#### Feature Matrix\n",
    "Machine learning algorithms are most easily defined in the language of linear algebra. Therefore, it will be useful to represent the entire dataset as one matrix $X \\in \\mathbb{R}^{n \\times d}$, of the form:\n",
    "$$ X = \\begin{bmatrix}\n",
    "x^{(1)}_1 & x^{(2)}_1 & \\ldots & x^{(n)}_1 \\\\\n",
    "x^{(1)}_2 & x^{(2)}_2 & \\ldots & x^{(n)}_2 \\\\\n",
    "\\vdots \\\\\n",
    "x^{(1)}_d & x^{(2)}_d & \\ldots & x^{(n)}_d\n",
    "\\end{bmatrix}.$$\n",
    "\n",
    "Similarly, we can vectorize the target variables into a vector $y \\in \\mathbb{R}^n$ of the form\n",
    "$$ y = \\begin{bmatrix}\n",
    "x^{(1)} \\\\\n",
    "x^{(2)} \\\\\n",
    "\\vdots \\\\\n",
    "x^{(n)}\n",
    "\\end{bmatrix}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8258e9af",
   "metadata": {},
   "source": [
    "# Lecture 3: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fd3cc0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Calculus Review: Derivatives\n",
    "\n",
    "Recall that the derivative $$\\frac{d f(\\theta_0)}{d \\theta}$$ of a univariate function $f : \\mathbb{R} \\to \\mathbb{R}$ is the instantaneous rate of change of the function $f(\\theta)$ with respect to its parameter $\\theta$ at the point $\\theta_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb52cb3c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Calculus Review: Partial Derivatives\n",
    "\n",
    "The partial derivative $$\\frac{\\partial f(\\theta)}{\\partial \\theta_j}$$ of a multivariate function $f : \\mathbb{R}^d \\to \\mathbb{R}$ is the derivative of $f$ with respect to $\\theta_j$ while all the other dimensions $\\theta_k$ for $k\\neq j$ are fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a06224a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Calculus Review: The Gradient\n",
    "\n",
    "The gradient $\\nabla f$ is the vector of all the partial derivatives:\n",
    "\n",
    "$$ \\nabla f (\\theta) = \\begin{bmatrix}\n",
    "\\frac{\\partial f(\\theta)}{\\partial \\theta_1} \\\\\n",
    "\\frac{\\partial f(\\theta)}{\\partial \\theta_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial f(\\theta)}{\\partial \\theta_d}\n",
    "\\end{bmatrix}.$$\n",
    "\n",
    "The $j$-th entry of the vector $\\nabla f (\\theta)$ is the partial derivative $\\frac{\\partial f(\\theta)}{\\partial \\theta_j}$ of $f$ with respect to the $j$-th component of $\\theta$.\n",
    "\n",
    "**The gradient, represented by the blue arrows, denotes the direction of greatest change of a scalar function.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e5c92e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Calculus Review: The Gradient\n",
    "\n",
    "The gradient $\\nabla_\\theta f$ further extends the derivative to multivariate functions $f : \\mathbb{R}^d \\to \\mathbb{R}$, and is defined at a point $\\theta_0$ as\n",
    "\n",
    "$$ \\nabla_\\theta f (\\theta_0) = \\begin{bmatrix}\n",
    "\\frac{\\partial f(\\theta_0)}{\\partial \\theta_1} \\\\\n",
    "\\frac{\\partial f(\\theta_0)}{\\partial \\theta_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial f(\\theta_0)}{\\partial \\theta_d}\n",
    "\\end{bmatrix}.$$\n",
    "\n",
    "The $j$-th entry of the vector $\\nabla_\\theta f (\\theta_0)$ is the partial derivative $\\frac{\\partial f(\\theta_0)}{\\partial \\theta_j}$ of $f$ with respect to the $j$-th component of $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552cf7bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient Descent: Intuition\n",
    "\n",
    "Gradient descent is a very common optimization algorithm used in machine learning.\n",
    "\n",
    "The intuition behind gradient descent is to repeatedly obtain the gradient to determine the direction in which the function decreases most steeply and take a step in that direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5988bd84",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient Descent: Notation\n",
    "More formally, if we want to optimize $J(\\theta)$, we start with an initial guess $\\theta_0$ for the parameters and repeat the following update until $\\theta$ is no longer changing:\n",
    "$$ \\theta_i := \\theta_{i-1} - \\alpha \\cdot \\nabla J(\\theta_{i-1}). $$\n",
    "\n",
    "In code, this method may look as follows:\n",
    "```python\n",
    "theta, theta_prev = random_initialization()\n",
    "while norm(theta - theta_prev) > convergence_threshold:\n",
    "    theta_prev = theta\n",
    "    theta = theta_prev - step_size * gradient(theta_prev)\n",
    "```\n",
    "In the above algorithm, we stop when $||\\theta_i - \\theta_{i-1}||$ is small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c409ed",
   "metadata": {},
   "source": [
    "# Resource on Gradient Descent: https://www.cnblogs.com/pinard/p/5970503.html  \n",
    "\n",
    "那么这个梯度向量求出来有什么意义呢？他的意义从几何意义上讲，就是函数变化增加最快的地方。具体来说，对于函数f(x,y),在点(x0,y0)，沿着梯度向量的方向就是(∂f/∂x0, ∂f/∂y0)T的方向是f(x,y)增加最快的地方。或者说，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向，也就是 -(∂f/∂x0, ∂f/∂y0)T的方向，梯度减少最快，也就是更加容易找到函数的最小值。\n",
    "\n",
    "在机器学习算法中，在最小化损失函数时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数，和模型参数值。反过来，如果我们需要求解损失函数的最大值，这时就需要用梯度上升法来迭代了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d18b4d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Review: Linear Model Family\n",
    "\n",
    "Recall that a linear model has the form\n",
    "\\begin{align*}\n",
    "y & = \\theta_0 + \\theta_1 \\cdot x_1 + \\theta_2 \\cdot x_2 + ... + \\theta_d \\cdot x_d\n",
    "\\end{align*}\n",
    "where $x \\in \\mathbb{R}^d$ is a vector of features and $y$ is the target. The $\\theta_j$ are the *parameters* of the model.\n",
    "\n",
    "By using the notation $x_0 = 1$, we can represent the model in a vectorized form\n",
    "$$ f_\\theta(x) = \\sum_{j=0}^d \\theta_j \\cdot x_j = \\theta^\\top x. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566558fd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# An Objective: Mean Squared Error\n",
    "\n",
    "We pick $\\theta$ to minimize the mean squared error (MSE). Slight variants of this objective are also known as the residual sum of squares (RSS) or the sum of squared residuals (SSR).\n",
    "$$J(\\theta)= \\frac{1}{2n} \\sum_{i=1}^n(y^{(i)}-\\theta^\\top x^{(i)})^2$$\n",
    "In other words, we are looking for the best compromise in $\\theta$ over all the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f2d828",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mean Squared Error: Partial Derivatives\n",
    "\n",
    "Let's work out the derivatives for $\\frac{1}{2} \\left( f_\\theta(x^{(i)}) - y^{(i)} \\right)^2,$ the MSE of a linear model $f_\\theta$ for one training example $(x^{(i)}, y^{(i)})$, which we denote $J^{(i)}(\\theta)$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial \\theta_j} J^{(i)}(\\theta) & = \\frac{\\partial}{\\partial \\theta_j} \\left(\\frac{1}{2} \\left( f_\\theta(x^{(i)}) - y^{(i)} \\right)^2\\right) \\\\\n",
    "& = \\left( f_\\theta(x^{(i)}) - y^{(i)} \\right) \\cdot \\frac{\\partial}{\\partial \\theta_j} \\left( f_\\theta(x^{(i)}) - y^{(i)} \\right) \\\\\n",
    "& = \\left( f_\\theta(x^{(i)}) - y^{(i)} \\right) \\cdot \\frac{\\partial}{\\partial \\theta_j} \\left( \\sum_{k=0}^d \\theta_k \\cdot x^{(i)}_k - y^{(i)} \\right) \\\\\n",
    "& = \\left( f_\\theta(x^{(i)}) - y^{(i)} \\right) \\cdot x^{(i)}_j\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f4b55a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mean Squared Error: The Gradient\n",
    "\n",
    "We can use this derivation to obtain an expression for the gradient of the MSE for a linear model\n",
    "\\begin{align*}\n",
    "\\small\n",
    "{\\tiny \\nabla_\\theta J^{(i)} (\\theta)} = \\begin{bmatrix}\n",
    "\\frac{\\partial J^{(i)}(\\theta)}{\\partial \\theta_0} \\\\\n",
    "\\frac{\\partial J^{(i)}(\\theta)}{\\partial \\theta_1} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial J^{(i)}(\\theta)}{\\partial \\theta_d}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\left( f_\\theta(x^{(i)}) - y^{(i)} \\right) \\cdot x^{(i)}_0 \\\\\n",
    "\\left( f_\\theta(x^{(i)}) - y^{(i)} \\right) \\cdot x^{(i)}_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\left( f_\\theta(x^{(i)}) - y^{(i)} \\right) \\cdot x^{(i)}_d\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\left( f_\\theta(x^{(i)}) - y^{(i)} \\right) \\cdot x^{(i)}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2105002",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that the MSE over the entire dataset is $J(\\theta) = \\frac{1}{n}\\sum_{i=1}^n J^{(i)}(\\theta)$. Therefore:\n",
    "\\begin{align*}\n",
    "\\nabla_\\theta J (\\theta) = \\begin{bmatrix}\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_0} \\\\\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_1} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_d}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\frac{1}{n}\\sum_{i=1}^n\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial J^{(i)}(\\theta)}{\\partial \\theta_0} \\\\\n",
    "\\frac{\\partial J^{(i)}(\\theta)}{\\partial \\theta_1} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial J^{(i)}(\\theta)}{\\partial \\theta_d}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\frac{1}{n} \\sum_{i=1}^n \\left( f_\\theta(x^{(i)}) - y^{(i)} \\right) \\cdot x^{(i)}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7de827c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient Descent for Linear Regression\n",
    "\n",
    "Putting this together with the gradient descent algorithm, we obtain a learning method for training linear models.\n",
    "\n",
    "\n",
    "```python\n",
    "theta, theta_prev = random_initialization()\n",
    "while abs(J(theta) - J(theta_prev)) > conv_threshold:\n",
    "    theta_prev = theta\n",
    "    theta = theta_prev - step_size * (f(x, theta)-y) * x\n",
    "```\n",
    "\n",
    "This update rule is also known as the Least Mean Squares (LMS) or Widrow-Hoff learning rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571c7e0b",
   "metadata": {},
   "source": [
    "# Algorithm: Ordinary Least Squares\n",
    "\n",
    "* __Type__: Supervised learning (regression)\n",
    "* __Model family__: Linear models\n",
    "* __Objective function__: Mean squared error\n",
    "* __Optimizer__: Normal equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4b3a93",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Notation: Design Matrix\n",
    "\n",
    "<!-- Suppose that we have a dataset of size $n$ (e.g., $n$ patients), indexed by $i=1,2,...,n$. Each $x^{(i)}$ is a vector of $d$ features. -->\n",
    "\n",
    "Machine learning algorithms are most easily defined in the language of linear algebra. Therefore, it will be useful to represent the entire dataset as one matrix $X \\in \\mathbb{R}^{n \\times d}$, of the form:\n",
    "\n",
    "$$ X = \\begin{bmatrix}\n",
    "x^{(1)}_1 & x^{(1)}_2 & \\ldots & x^{(1)}_d \\\\\n",
    "x^{(2)}_1 & x^{(2)}_2 & \\ldots & x^{(2)}_d \\\\\n",
    "\\vdots \\\\\n",
    "x^{(n)}_1 & x^{(n)}_2 & \\ldots & x^{(n)}_d\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "- & (x^{(1)})^\\top & - \\\\\n",
    "- & (x^{(2)})^\\top & - \\\\\n",
    "& \\vdots & \\\\\n",
    "- & (x^{(n)})^\\top & - \\\\\n",
    "\\end{bmatrix}\n",
    ".$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf60efc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Squared Error in Matrix Form\n",
    "\n",
    "Recall that we may fit a linear model by choosing $\\theta$ that minimizes the squared error:\n",
    "$$J(\\theta)=\\frac{1}{2}\\sum_{i=1}^n(y^{(i)}-\\theta^\\top x^{(i)})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b77aa7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can write this sum in matrix-vector form as:\n",
    "$$J(\\theta) = \\frac{1}{2} (y-X\\theta)^\\top(y-X\\theta) = \\frac{1}{2} \\|y-X\\theta\\|^2,$$\n",
    "where $X$ is the design matrix and $\\|\\cdot\\|$ denotes the Euclidean norm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df0a6a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Gradient of the Squared Error\n",
    "\n",
    "We can compute the gradient of the mean squared error as follows.\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_\\theta J(\\theta) \n",
    "& = \\nabla_\\theta \\frac{1}{2} (X \\theta - y)^\\top  (X \\theta - y) \\\\\n",
    "& = \\frac{1}{2} \\nabla_\\theta \\left( (X \\theta)^\\top  (X \\theta) - (X \\theta)^\\top y - y^\\top (X \\theta) + y^\\top y \\right) \\\\\n",
    "& = \\frac{1}{2} \\nabla_\\theta \\left( \\theta^\\top  (X^\\top X) \\theta - 2(X \\theta)^\\top y \\right) \\\\\n",
    "& = \\frac{1}{2} \\left( 2(X^\\top X) \\theta - 2X^\\top y \\right) \\\\\n",
    "& = (X^\\top X) \\theta - X^\\top y\n",
    "\\end{align*}\n",
    "\n",
    "We used the facts that $a^\\top b = b^\\top a$ (line 3), that $\\nabla_x b^\\top x = b$ (line 4), and that $\\nabla_x x^\\top A x = 2 A x$ for a symmetric matrix $A$ (line 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea24329",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Normal Equations\n",
    "\n",
    "<!-- We know from calculus that a function is minimized when its derivative is set to zero. In our case, our objective function is a (multivariate) quadratic; hence it only has one minimum, which is the global minimum.\n",
    " -->\n",
    "Setting the above derivative to zero, we obtain the *normal equations*:\n",
    "$$ (X^\\top X) \\theta = X^\\top y.$$\n",
    "\n",
    "Hence, the value $\\theta^*$ that minimizes this objective is given by:\n",
    "$$ \\theta^* = (X^\\top X)^{-1} X^\\top y.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85396ce8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Algorithm: Non-Linear Least Squares\n",
    "\n",
    "* __Type__: Supervised learning (regression)\n",
    "* __Model family__: Linear in the parameters; non-linear with respect to raw inputs.\n",
    "* __Features__: Non-linear functions of the attributes\n",
    "* __Objective function__: Mean squared error\n",
    "* __Optimizer__: Normal equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e5ddf9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modeling Non-Linear Relationships With Polynomial Regression\n",
    "\n",
    "<!-- Note that the set of $p$-th degree polynomials forms a linear model with parameters $a_p, a_{p-1}, ..., a_0$.\n",
    "This means we can use our algorithms for linear models to learn non-linear features! -->\n",
    "\n",
    "Specifically, given a one-dimensional continuous variable $x$, we can define the *polynomial feature* function $\\phi : \\mathbb{R} \\to \\mathbb{R}^{p+1}$ as\n",
    "$$ \\phi(x) = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "x \\\\\n",
    "x^2 \\\\\n",
    "\\vdots \\\\\n",
    "x^p\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef5d29a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The class of models of the form\n",
    "$$ f_\\theta(x) := \\sum_{j=0}^p \\theta_p x^p = \\theta^\\top \\phi(x) $$\n",
    "with parameters $\\theta$ and polynomial features $\\phi$ is the set of $p$-degree polynomials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5bc7fa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* This model is non-linear in the input variable $x$, meaning that we can model complex data relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eede414a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* It is a linear model as a function of the parameters $\\theta$, meaning that we can use our familiar ordinary least squares algorithm to learn these features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30835c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multivariate Polynomial Regression\n",
    "\n",
    "We can construct non-linear functions of multiple variables by using multivariate polynomials.\n",
    "\n",
    "For example, a polynomial of degree $2$ over two variables $x_1, x_2$ is a function of the form\n",
    "<!-- $$\n",
    "a_{20} x_1^2 + a_{10} x_1 + a_{02} x_2^2 + a_{01} x_2 + a_{22} x_1^2 x_2^2 + a_{21} x_1^2 x_2 + a_{12} x_1 x_2^2 + a_11 x_1 x_2 + a_{00}.\n",
    "$$ -->\n",
    "$$\n",
    "a_{20} x_1^2 + a_{10} x_1 + a_{02} x_2^2 + a_{01} x_2 + a_{11} x_1 x_2 + a_{00}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f0ab64",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In general, a polynomial of degree $p$ over two variables $x_1, x_2$ is a function of the form\n",
    "$$\n",
    "f(x_1, x_2) = \\sum_{i,j \\geq 0 : i+j \\leq p} a_{ij} x_1^i x_2^j.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af8e803",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In our two-dimensional example, this corresponds to a feature function $\\phi : \\mathbb{R}^2 \\to \\mathbb{R}^6$ of the form\n",
    "$$ \\phi(x) = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "x_1 \\\\\n",
    "x_1^2 \\\\\n",
    "x_2 \\\\\n",
    "x_2^2 \\\\\n",
    "x_1 x_2\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "The same approach can be used to specify polynomials of any degree and over any number of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44481306",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Towards General Non-Linear Features\n",
    "\n",
    "Any non-linear feature map $\\phi(x) : \\mathbb{R}^d \\to \\mathbb{R}^p$ can be used in this way to obtain general models of the form\n",
    "$$ f_\\theta(x) := \\theta^\\top \\phi(x) $$\n",
    "that are highly non-linear in $x$ but linear in $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad90bb3e",
   "metadata": {
    "colab_type": "text",
    "id": "i_f5u2x9nn6I",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<left><img width=25% src=\"img/cornell_tech2.svg\"></left>\n",
    "\n",
    "# Lecture 4: Classification and Logistic Regression\n",
    "\n",
    "### Applied Machine Learning\n",
    "\n",
    "__Volodymyr Kuleshov__<br>Cornell Tech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5476f0a8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Review: Regression vs. Classification\n",
    "\n",
    "Consider a training dataset $\\mathcal{D} = \\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\ldots, (x^{(n)}, y^{(n)})\\}$.\n",
    "\n",
    "We distinguish between two types of supervised learning problems depnding on the targets $y^{(i)}$. \n",
    "\n",
    "1. __Regression__: The target variable $y \\in \\mathcal{Y}$ is continuous:  $\\mathcal{Y} \\subseteq \\mathbb{R}$.\n",
    "2. __Classification__: The target variable $y$ is discrete and takes on one of $K$ possible values:  $\\mathcal{Y} = \\{y_1, y_2, \\ldots y_K\\}$. Each discrete value corresponds to a *class* that we want to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d90298",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Binary Classification\n",
    "\n",
    "An important special case of classification is when the number of classes $K=2$.\n",
    "\n",
    "In this case, we have an instance of a *binary classification* problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3348a1be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Understanding Classification\n",
    "\n",
    "How is clasification different from regression?\n",
    "* In regression, we try to fit a curve through the set of targets $y^{(i)}$.\n",
    "* In classification, classes define a partition of the feature space, and our goal is to find the boundaries that separate these regions.\n",
    "* Outputs of classification models often have a simple probabilistic interpretation: they are probabilities that a data point belongs to a given class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00e046a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Review: Least Squares\n",
    "\n",
    "Recall that the linear regression algorithm fits a linear model of the form\n",
    "$$ f(x) = \\sum_{j=0}^d \\theta_j \\cdot x_j = \\theta^\\top x. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12ad125",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It minimizes the mean squared error (MSE)\n",
    "$$J(\\theta)= \\frac{1}{2n} \\sum_{i=1}^n(y^{(i)}-\\theta^\\top x^{(i)})^2$$\n",
    "on a dataset $\\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\ldots, (x^{(n)}, y^{(n)})\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d16d35",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We could use least squares to solve our classification problem, setting $\\mathcal{Y} = \\{0, 1\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65181be",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ordinary least squares returns a decision boundary that is not unreasonable.\n",
    "\n",
    "However, applying ordinary least squares is problematic for a few reasons.\n",
    "* There is nothing to prevent outputs larger than one or smaller than zero, which is conceptually wrong\n",
    "* We also don't have optimal performance: at least one point is misclassified, and others are too close to the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971d5589",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Logistic Function\n",
    "\n",
    "To address the fact that the output of linear regression is not in $[0,1]$, we will instead attempt to *squeeze* it into that range using\n",
    "$$ \\sigma(z) = \\frac{1}{1 + \\exp(-z)}. $$\n",
    "This is known as the *sigmoid* or *logistic* function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b994971",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The logistic function $\\sigma : \\mathbb{R} \\to [0,1]$ \"squeezes\" points from the real line into $[0,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b718c7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Logistic Function: Properties\n",
    "\n",
    "The sigmoid function is defined as\n",
    "$$ \\sigma(z) = \\frac{1}{1 + \\exp(-z)}. $$\n",
    "A few observations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cc4dec",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The function tends to 1 as $z \\to \\infty$ and tends to 0 as $z \\to -\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a51ae0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Thus, models of the form $\\sigma(\\theta^\\top x)$ output values between 0 and 1, which is suitable for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5246e210",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* It is easy to show that the derivative of $\\sigma(z)$ has a simple form:\n",
    "$\\frac{d\\sigma}{dz} = \\sigma(z)(1-\\sigma(z)).$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff4e682",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that logistic regression is actually a binary __classification__ algorithm.\n",
    "\n",
    "The term *regression* is an unfortunate historical misnomer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d76c3f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probabilistic Interpretations\n",
    "\n",
    "The logistic model can be interpreted to output a probability, and defines a conditional probability distribution as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "P_\\theta(y=1 | x) & = \\sigma(\\theta^\\top x) \\\\\n",
    "P_\\theta(y=0 | x) & = 1-\\sigma(\\theta^\\top x).\n",
    "\\end{align*}\n",
    "\n",
    "Recall that a probability over $y\\in \\{0,1\\}$ is called **Bernoulli.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1273b8e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# The Data Distribution\n",
    "\n",
    "We will assume that our dataset is sampled from a probability distribution $\\mathbb{P}$, which we will call the *data distribution*. We will denote this as\n",
    "$$ x, y \\sim P_\\text{data}. $$\n",
    "\n",
    "The training set $\\mathcal{D} = \\{(x^{(i)}, y^{(i)}) \\mid i = 1,2,...,n\\}$ consists of *independent and identicaly distributed* (IID) samples from $P_\\text{data}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccc59a6",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Data Distribution: IID Sampling\n",
    "\n",
    "The key assumption is that the training examples are *independent and identicaly distributed* (IID). \n",
    "* Each training example is from the same distribution.\n",
    "* This distribution doesn't depend on previous training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ba269a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "__Example__: Flipping a coin. Each flip has same probability of heads & tails and doesn't depend on previous flips."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d56a98c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "__Counter-Example__: Yearly census data. The population in each year will be close to that of the previous year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d927b987",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Data Distribution: Motivation\n",
    "\n",
    "Why assume that the dataset is sampled from a distribution?\n",
    "\n",
    "* The process we model may be effectively random. If $y$ is a stock price, there is randomness in the market that cannot be captured by a deterministic model.\n",
    "* There may be noise and randomness in the data collection process itself (e.g., collecting readings from an imperfect thermometer).\n",
    "* We can use probability and statistics to analyze supervised learning algorithms and prove that they work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f10693",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probabilistic Interpretations\n",
    "\n",
    "Many supervised learning models have a probabilistic interpretation.\n",
    "Often a model $f_\\theta$ defines a probability distribution of the form\n",
    "\n",
    "\\begin{align*}\n",
    "P_\\theta(x,y) : \\mathcal{X} \\times \\mathcal{Y} \\to [0,1] && \\text{or} && P_\\theta(y|x) : \\mathcal{X} \\times \\mathcal{Y} \\to [0,1].\n",
    "\\end{align*}\n",
    "\n",
    "We refer to these as *probabilistic models*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffb088e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example, our logistic model defines (\"parameterizes\") a probability distribution $P_\\theta(y|x) : \\mathcal{X} \\times \\mathcal{Y} \\to [0,1]$ as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "P_\\theta(y=1 | x) & = \\sigma(\\theta^\\top x) \\\\\n",
    "P_\\theta(y=0 | x) & = 1-\\sigma(\\theta^\\top x).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbbca8e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Maximum Likelihood\n",
    "\n",
    "We can train any model that defines a probability distribution $P_\\theta(x,y)$ by optimizing\n",
    "the *maximum likelihood* objective\n",
    "$$\n",
    "L(\\theta) = \\prod_{i=1}^n P_\\theta({x}^{(i)}, y^{(i)})\n",
    "$$\n",
    "defined over a dataset $\\mathcal{D} = \\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\ldots, (x^{(n)}, y^{(n)})\\}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a1bf68",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This asks that $P_\\theta$ assign a high probability to the training instances in the dataset $\\mathcal{D}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a20263",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A lot of mathematical derivations and numerical calculations become simpler if we instead maximize the log of the likelihood:\n",
    "\\begin{align*}\n",
    "\\ell(\\theta) = \\log L(\\theta) & = \\log \\prod_{i=1}^n P_\\theta({x}^{(i)}, y^{(i)}) = \\sum_{i=1}^n \\log P_\\theta({x}^{(i)}, y^{(i)}).\n",
    "\\end{align*}\n",
    "Note that since the log is a monotonically increasing function, it doesn't change the optimal $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b4f3ba",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Finally, it's often simpler to take the average, instead of the sum. This gives us the following learning principle, known as *maximum log-likelihood*:\n",
    "$$\n",
    "\\max_\\theta \\ell(\\theta) = \\max_{\\theta} \\frac{1}{n}\\sum_{i=1}^n \\log P_\\theta({x}^{(i)}, y^{(i)}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b86019",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Example: Flipping a Random Coin\n",
    "\n",
    "Our log-likelihood function is\n",
    "\\begin{eqnarray}\n",
    "\\nonumber\n",
    "L(\\theta) &=& \\theta^{\\#\\,\\text{heads}} \\cdot (1-\\theta)^{\\#\\,\\text{tails}}\\\\\n",
    "\\nonumber\n",
    "\\log L(\\theta) &=& \\log(\\theta^{\\#\\,\\text{heads}} \\cdot (1-\\theta)^{\\#\\,\\text{tails}}) \\\\\n",
    "\\nonumber\n",
    "&=& \\#\\,\\text{heads} \\cdot \\log(\\theta) + \\#\\,\\text{tails} \\cdot \\log(1-\\theta)\n",
    "\\end{eqnarray}\n",
    "The maximum likelihood estimate is the $\\theta^* \\in [0,1]$ such that $\\log L(\\theta^*)$ is maximized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40642558",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Differentiating the log-likelihood function with respect to $\\theta$ and setting the derivative to zero, we obtain\n",
    "$$\\theta^*= \\frac{\\#\\,\\text{heads}}{\\#\\,\\text{heads}+\\#\\,\\text{tails}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9478d04",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conditional Maximum Likelihood\n",
    "\n",
    "Often, a machine learning model defines a *conditional* model $P_\\theta(y | x)$. In this setting, we can maximize the *conditional maximum likelihood*:\n",
    "\n",
    "$$ \\max_\\theta \\frac{1}{n}\\sum_{i=1}^n \\log P_\\theta(y^{(i)}|{x}^{(i)}). $$\n",
    "\n",
    "**This asks that for each input $x^{(i)}$ in the dataset $\\mathcal{D}$, $P_\\theta$ should assign a high probability to the correct target $y^{(i)}$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0871ecea",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# KL Divergences and Max Likelihood\n",
    "\n",
    "Maximizing likelihood is closely related to minimizing the Kullback-Leibler (KL) divergence $D(\\cdot\\|\\cdot)$ between the model distribution and the data distribution.\n",
    "$$\n",
    "D(p \\| q) = \\sum_{{\\bf x}} p({\\bf x}) \\log \\frac{p({\\bf x})}{q({\\bf x})}.\n",
    "$$\n",
    "The KL divergence is always non-negative, and equals zero when $p$ and $q$ are identical. This makes it a natural measure of similarity that's useful for comparing distributions. \n",
    "\n",
    "在统计学意义上来说，KL散度可以用来衡量两个分布之间的差异程度。若两者差异越小，KL散度越小，反之亦反。当两分布一致时，其KL散度为0。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27b51bd",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let $\\hat{P}(x, y)$ denote the *empirical* distribution of the data:\n",
    "\n",
    "$$\n",
    "\\hat{P}(x, y) = \\begin{cases}\n",
    "\\frac{1}{n} & \\text{if } (x,y) \\in \\mathcal{D} \\\\\n",
    "0 & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This distribution assigns a probability of $1/n$ to each of the data points in $\\mathcal{D}$ (and zero to all the other possible $(x,y)$); it can be seen as a guess of the true data distribution from which the dataset $\\mathcal{D}$ was obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968f1cd1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Selecting parameters $\\theta$ by maximizing the likelihood is equivalent to selecting $\\theta$ that minimizes the KL divergence between the empirical data distribution and the model distribution:\n",
    "\n",
    "\\begin{align*}\n",
    "\\max_{\\theta} \\frac{1}{n} \\sum_{i=1}^n \\log P_\\theta({x}^{(i)}, y^{(i)})\n",
    "& = \\max_{\\theta} \\mathbb{E}_{\\hat{P}(x, y)} \\left[ \\log P_\\theta(x,y) \\right] \\\\\n",
    "& = \\min_{\\theta} KL(\\hat{P}(x,y) || P_\\theta(x,y) ).\n",
    "\\end{align*}\n",
    "\n",
    "Here, $\\mathbb{E}_{p(x)} f(x)$ denotes $\\sum_{x \\in \\mathcal{X}} f(x) p(x)$ if $x$ is discrete and $\\int_{x \\in \\mathcal{X}} f(x) p(x) dx$ if $x$ is continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aa50f8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The same is true for conditional log-likelihood (homework problem!):\n",
    "\n",
    "$$\n",
    "\\max_{\\theta} \\mathbb{E}_{\\hat{P}(x, y)} \\left[ \\log P_\\theta(y|x) \\right] = \\min_{\\theta} \\mathbb{E}_{\\hat{P}(x)} \\left[ KL(\\hat{P}(y|x) || P_\\theta(y|x) )\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e28725",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recall: Ordinary Least Squares\n",
    "\n",
    "Recall that in ordinary least squares (OLS), we have a linear model of the form\n",
    "$$f(x) = \\sum_{j=0}^d \\theta_j \\cdot x_j = \\theta^\\top x.$$\n",
    "\n",
    "At each training instance $(x,y)$, we seek to minimize the squared error\n",
    "$$(y - \\theta^\\top x)^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4ccf2b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Maximum Likelihood Interpretation of OLS\n",
    "\n",
    "Let's make our usual linear regression model probabilistic: assume that the targets and the inputs are related by\n",
    "\n",
    "$$ y = \\theta^\\top x + \\epsilon, $$\n",
    "\n",
    "where $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ is a random noise term that follows a Gaussian (or \"Normal\") distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edf73ff",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The density of $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ is a Gaussian distribution:\n",
    "$$ P(\\epsilon; \\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left( -\\frac{\\epsilon^2}{2 \\sigma^2} \\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973bb9b2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This implies that\n",
    "$$ P(y | x; \\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left( -\\frac{(y - \\theta^\\top x)^2}{2 \\sigma^2} \\right).$$\n",
    "This is a Gaussian distribution with mean $\\mu_\\theta(x) = \\theta^\\top x$ and variance $\\sigma^2$. \n",
    "\n",
    "Given an input of $x$, this model outputs a \"mini Bell curve\" with width $\\sigma$ around the mean $\\mu(x) = \\sigma^\\top x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7560f4f8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's now learn the parameters $\\theta$ of \n",
    "$$ P(y | x; \\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left( -\\frac{(y - \\theta^\\top x)^2}{2 \\sigma^2} \\right)$$\n",
    "using maximum likelihood. \n",
    "\n",
    "The log-likelihood of this model at a point $(x,y)$ equals\n",
    "\n",
    "\\begin{align*}\n",
    "\\log L(\\theta) = \\log p(y | x; \\theta) = \\text{const}_1 \\cdot (y - \\theta^\\top x)^2 + \\text{const}_2\n",
    "\\end{align*}\n",
    "\n",
    "for some constants $\\text{const}_1, \\text{const}_2$. But that's just the least squares objective!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5285b55",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Least squares thus amounts to fitting a Gaussian  model $\\mathcal{N}(y; \\mu(x), \\sigma)$ with a standard deviation $\\sigma$ of one and a mean of $\\mu(x) = \\theta^\\top x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be39bc37",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note in particular that **OLS implicitly makes the assumption that the noise is Gaussian around the mean.** \n",
    "\n",
    "When that's not the case, you may want to also experiment with other kinds of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c1cc4f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Recall that a logistic model defines (\"parameterizes\") a probability distribution $P_\\theta(y|x) : \\mathcal{X} \\times \\mathcal{Y} \\to [0,1]$ as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "P_\\theta(y=1 | x) & = \\sigma(\\theta^\\top x) \\\\\n",
    "P_\\theta(y=0 | x) & = 1-\\sigma(\\theta^\\top x).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104ff82f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When $y \\in \\{0,1\\}$, can write this more compactly as\n",
    "\\begin{align*}\n",
    "P_\\theta(y | x) = \\sigma(\\theta^\\top x)^y \\cdot (1-\\sigma(\\theta^\\top x))^{1-y}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f731b7f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Applying Maximum Lilkelihood - Log Loss\n",
    "\n",
    "Following the principle of maximum likelihood, we want to optimize the following objective defined over a binary classification dataset  $\\mathcal{D} = \\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\ldots, (x^{(n)}, y^{(n)})\\}$.\n",
    "\\begin{align*}\n",
    "\\ell(\\theta) & = \\frac{1}{n}\\sum_{i=1}^n \\log P_\\theta (y^{(i)} \\mid x^{(i)}) \\\\\n",
    "& = \\frac{1}{n}\\sum_{i=1}^n \\log \\sigma(\\theta^\\top x^{(i)})^{y^{(i)}} \\cdot (1-\\sigma(\\theta^\\top x^{(i)}))^{1-y^{(i)}} \\\\\n",
    "& = \\frac{1}{n}\\sum_{i=1}^n {y^{(i)}} \\cdot \\log \\sigma(\\theta^\\top x^{(i)}) + (1-y^{(i)}) \\cdot \\log (1-\\sigma(\\theta^\\top x^{(i)})).\n",
    "\\end{align*}\n",
    "\n",
    "This objective is also often called the **log-loss, or cross-entropy.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed652d5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Observe that this objective asks the model to ouput a large score $\\sigma(\\theta^\\top x^{(i)})$ (a score that's close to one) if $y^{(i)}=1$, and a score that's small (close to zero) if $y^{(i)}=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477c9e30",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient of the Log-Likelihood\n",
    "\n",
    "We want to use gradient descent to maximize the log-likelihood, hence our objective is\n",
    "$J(\\theta) = - \\ell(\\theta).$\n",
    "\n",
    "We can show that the gradient of the negative log-likelihood equals:\n",
    "\\begin{align*}\n",
    "\\nabla_\\theta J (\\theta) =  \\nabla_\\theta \\left[-\\ell (\\theta) \\right]= \n",
    "\\left( \\sigma(\\theta^\\top x) - y \\right) \\cdot \\bf{x}.\n",
    "\\end{align*}\n",
    "\n",
    "Interestingly, this expression looks similar to the gradient of the mean squared error, which we derived in the previous lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ade52b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Algorithm: Logistic Regression\n",
    "\n",
    "* __Type__: Supervised learning (binary classification)\n",
    "* __Model family__: Linear decision boundaries.\n",
    "* __Objective function__: Cross-entropy, a special case of log-likelihood.\n",
    "* __Optimizer__: Gradient descent.\n",
    "* __Probabilistic interpretation__: Parametrized Bernoulli distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b67c039",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Observations About Logistic Regression\n",
    "\n",
    "* Logistic regression finds a linear decision boundary. This is the set of points for which $P(y=1|x)=P(y=0|x)$, or equivalently:\n",
    "\\begin{align*}\n",
    "0 = \\log\\frac{P(y=1|x)}{P(y=0|x)}\n",
    "= \\log \\frac{\\frac{1}{1+\\exp(-\\theta^\\top x)}}{1-\\frac{1}{1+\\exp(-\\theta^\\top x)}}\n",
    "= \\theta^\\top x\n",
    "\\end{align*}\n",
    "The set of $x$ for which $0=\\theta^\\top x$ is a linear surface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd50d5b6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Unlike least squares, we don't have a closed form solution (a formula) for the optimal $\\theta$. We can nonetheless find it numerically via gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7258f7c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multi-Class Classification\n",
    "\n",
    "Logistic regression only applies to binary classification problems. What if we have an arbitrary number of classes $K$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a77455e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Algorithm: Softmax Regression\n",
    "\n",
    "* __Type__: Supervised learning (multi-class classification)\n",
    "* __Model family__: Linear decision boundaries.\n",
    "* __Objective function__: Softmax loss, a special case of log-likelihood.\n",
    "* __Optimizer__: Gradient descent.\n",
    "* __Probabilistic interpretation__: Parametrized categorical distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf187a9b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Softmax Function\n",
    "\n",
    "The logistic function $\\sigma : \\mathbb{R} \\to [0,1]$ \"squeezes\" the score $z\\in\\mathbb{R}$ of a class into a probability in $[0,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1743498",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The *softmax* function $\\vec \\sigma : \\mathbb{R}^K \\to [0,1]^K$ is a multi-class version of $\\sigma$ \n",
    "* It takes in a $K$-dimensional *vector* of class scores $\\vec z\\in\\mathbb{R}$ \n",
    "* It \"squeezes\" $\\vec z$ into a length $K$ *vector* of  probabilities in $[0,1]^K$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfb2ab7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The $k$-th component of the output of the softmax function $\\vec \\sigma$ is defined as\n",
    "$$ \\sigma(\\vec z)_k = \\frac{\\exp(z_k)}{\\sum_{l=1}^K \\exp(z_l)}. $$\n",
    "\n",
    "Softmax takes a vector of scores $\\vec z$, exponentiates each score $z_k$, and normalizes the exponentiated scores such that they sum to one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3f142d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When $K=2$, this looks as follows:\n",
    "$$ \\sigma(\\vec z)_1 = \\frac{\\exp(z_1)}{\\exp(z_1) + \\exp(z_2)}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c366288",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Observe that adding a constant $c \\in \\mathbb{R}$ to each score $z_k$ doesn't change the output of softmax, e.g.:\n",
    "$$ \\frac{\\exp(z_1)}{\\exp(z_1) + \\exp(z_2)} = \\frac{\\exp(z_1+c)}{\\exp(z_1+c) + \\exp(z_2+c)}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46fd51a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Without loss of generality, we can assume $z_1=0$. For any $\\vec z = (z_1, z_2)$, we can define $\\vec z' = (0, z_2') = (0, z_2-z_1)$ such that $\\vec\\sigma(\\vec z) = \\vec\\sigma(\\vec z')$. Assuming $z_1=0$ doesn't change the probabilities that $\\vec\\sigma$ can output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27af4303",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Assuming that $z_1 =0$ means that $\\exp(z_1) = 1$ and softmax becomes\n",
    "$$ \\sigma(\\vec z)_1 = \\frac{1}{1 + \\exp(z_2)}. $$\n",
    "This is effectively our sigmoid function. Hence softmax generalizes the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb369ba0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Softmax Regression: Model Class\n",
    "\n",
    "Softmax regression is a multi-class classification algorithm which uses a model $f_\\theta : \\mathcal{X} \\to [0,1]^K$ that generalizes logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22114ca",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Softmax regression works as follows:\n",
    "1. Given an input $x$, we compute $K$ scores, one per class. The score \n",
    "$$z_k = \\theta_k^\\top x$$\n",
    "of class $k$ is a linear function of $x$ and parameters $\\theta_k$ for class $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728b3e2d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. We \"squeeze\" the vector of scores $\\vec z$ into $[0,1]^K$ using the softmax function $\\vec\\sigma$ and we output $\\vec\\sigma(\\vec z)$, a vector of $K$ probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b15d9a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The parameters of this model are $\\theta = (\\theta_1, \\theta_2, ..., \\theta_K)$, and the parameter space is $\\Theta = \\mathbb{R}^{K \\times d}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb4c3a3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The output of the model is a *vector* of class membership probabilities, whose $k$-th component $f_\\theta(x)_k$ is\n",
    "$$ f_\\theta(x)_k = \\sigma(\\theta_k^\\top x)_k = \\frac{\\exp(\\theta_k^\\top x)}{\\sum_{l=1}^K \\exp(\\theta_l^\\top x)}, $$\n",
    "where each $\\theta_l \\in \\mathbb{R}^d$  is the vector of parameters for class $\\ell$ and $\\theta = (\\theta_1, \\theta_2, ..., \\theta_K)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cfbf11",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**This model is again over-parametrized: adding a constant $c \\in \\mathbb{R}$ to every score $\\theta_k^\\top x$ does not change the output of the model.**\n",
    "\n",
    "Over-parametrization (which means having more model parameters than necessary) means that we are fitting a richer model than necessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0725a438",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As before, we can assume without loss of generality that $z_1=0$ (or equivalently that $\\theta_1=0$). This doesn't change the set of functions $\\mathcal{X} \\to [0,1]^K$ that our model class can represent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbe1af9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note again that softmax regression is actually a __classification__ algorithm.\n",
    "\n",
    "The term *regression* is an unfortunate historical misnomer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c92f385",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Softmax Regression: Probabilistic Interpretation - Parametrized Categorical Distribution\n",
    "\n",
    "The softmax model outputs a vector of probabilities, and defines a conditional probability distribution as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "P_\\theta(y=k | x) & = \\vec\\sigma(\\vec z)_k =\\frac{\\exp(\\theta_k^\\top x)}{\\sum_{l=1}^K \\exp(\\theta_l^\\top x)}.\n",
    "\\end{align*}\n",
    "\n",
    "Recall that a probability over $y\\in \\{1,2,...,K\\}$ is called Categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a108db",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Softmax Regression: Learning Objective\n",
    "\n",
    "We again maximize likelihood over a dataset  $\\mathcal{D}$.\n",
    "\\begin{align*}\n",
    "L(\\theta) & = \\prod_{i=1}^n P_\\theta (y^{(i)} \\mid x^{(i)}) = \\prod_{i=1}^n \\vec \\sigma(\\vec z^{(i)})_{y^{(i)}} \\\\\n",
    "& = \\prod_{i=1}^n \\left( \\frac{\\exp(\\theta_{y^{(i)}}^\\top x^{(i)})}{\\sum_{l=1}^K \\exp(\\theta_l^\\top x^{(i)})} \\right). \\\\\n",
    "\\end{align*}\n",
    "\n",
    "We optimize this using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f359c7",
   "metadata": {
    "colab_type": "text",
    "id": "i_f5u2x9nn6I",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<left><img width=25% src=\"img/cornell_tech2.svg\"></left>\n",
    "\n",
    "# Lecture 5: Foundations of Supervised Learning\n",
    "\n",
    "### Applied Machine Learning\n",
    "\n",
    "__Volodymyr Kuleshov__<br>Cornell Tech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eb408c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What Is A Good Supervised Learning Model?\n",
    "\n",
    "A good predictive model is one that makes __accurate predictions__ on __new data__ that it has not seen at training time.\n",
    "\n",
    "* Accurate object detection in new scenes\n",
    "* Correct translation of new sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6b5474",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Holdout Dataset: Definition\n",
    "\n",
    "A holdout dataset \n",
    "\n",
    "$$\\dot{\\mathcal{D}} = \\{(\\dot{x}^{(i)}, \\dot{y}^{(i)}) \\mid i = 1,2,...,m\\}$$\n",
    "\n",
    "is sampled IID from the same distribution $\\mathbb{P}$, and is distinct from the training dataset $\\mathcal{D}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfcd89f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Performance on a Holdout Set\n",
    "\n",
    "Intuitively, a supervised model $f_\\theta$ is successful if it performs well on a holdout set $\\dot{\\mathcal{D}}$ according to some measure\n",
    "\n",
    "$$\n",
    "\\frac{1}{m} \\sum_{i=1}^m L\\left(\\dot y^{(i)}, f_\\theta(\\dot x^{(i)}) \\right).\n",
    "$$\n",
    "\n",
    "**Here, $L : \\mathcal{X}\\times\\mathcal{Y} \\to \\mathbb{R}$ is a performance metric or a loss function that we get to choose.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23504f2e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The choice of the performance metric $L$ depends on the specific problem and our goals:\n",
    "* $L$ can be the training objective: mean squared error, cross-entropy\n",
    "* In classification, $L$ is often just accuracy: is $\\dot y^{(i)} = f_\\theta(\\dot x^{(i)})$?\n",
    "* $L$ can also implement a task-specific metric: $R^2$ metric (see Homework 1) for regression, F1 score for document retrieval, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89085c43",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, in a classification setting, we may be interested in the accuracy of the model. Thus, we want the % of misclassified inputs\n",
    "$$\n",
    "\\frac{1}{m} \\sum_{i=1}^m \\mathbb{I}\\left(\\dot y^{(i)} \\neq f_\\theta(\\dot x^{(i)}) \\right)\n",
    "$$\n",
    "to be small. \n",
    "\n",
    "Here $\\mathbb{I}\\left( A \\right)$ is an *indicator* function that equals one if $A$ is true and zero otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f71bde7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In practice, there exist many different holdout sets $\\dot{\\mathcal{D}}$, and we want our model to perform well on all of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392ecdff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Performance on Out-of-Distribution Data\n",
    "\n",
    "Intuitively, a supervised model $f_\\theta$ is successful if it performs well in expectation on new data $\\dot x, \\dot y$ sampled from the data distribution $\\mathbb{P}$:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{(\\dot x, \\dot y)\\sim \\mathbb{P}} \\left[ L\\left(\\dot y, f_\\theta(\\dot x \\right)) \\right] \\text{ is \"good\"}.\n",
    "$$\n",
    "\n",
    "Here, $L : \\mathcal{X}\\times\\mathcal{Y} \\to \\mathbb{R}$ is a performance metric and we take its expectation or average over all the possible samples $\\dot x, \\dot y$ from $\\mathbb{P}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cfc96f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall that formally, an expectation $\\mathbb{E}_{x)\\sim {P}} f(x)$ is $\\sum_{x \\in \\mathcal{X}} f(x) P(x)$ if $x$ is discrete and $\\int_{x \\in \\mathcal{X}} f(x) P(x) dx$ if $x$ is continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98c1b54",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Intuitively,\n",
    "$$\\mathbb{E}_{(\\dot x, \\dot y)\\sim \\mathbb{P}} \\left[ L\\left(\\dot y, f_\\theta(\\dot x) \\right) \\right]\n",
    "= \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} L\\left(y, f_\\theta(x) \\right) \\mathbb{P}(x, y)\n",
    "$$\n",
    "is the performance on an *infinite-sized* holdout set, where we have sampled every possible point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd9187d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In practice, we cannot measure\n",
    "$$\\mathbb{E}_{(\\dot x, \\dot y)\\sim \\mathbb{P}} \\left[ L\\left(\\dot y, f_\\theta(\\dot x) \\right) \\right]$$\n",
    "on infinite data. \n",
    "\n",
    "We approximate its performance with a sample $\\dot{\\mathcal{D}}$ from $\\mathbb{P}$ and we measure\n",
    "$$\n",
    "\\frac{1}{m} \\sum_{i=1}^m L\\left(\\dot y^{(i)}, f_\\theta(\\dot x^{(i)}) \\right).\n",
    "$$\n",
    "If the number of IID samples $m$ is large, this approximation holds (we call this a Monte Carlo approximation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d592e4ae",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, in a classification setting, we may be interested in the accuracy of the model. \n",
    "\n",
    "We want a small probability of making an error on a new $\\dot x, \\dot y \\sim \\mathbb{P}$:\n",
    "$$\n",
    "\\mathbb{P}\\left(\\dot y \\neq f_\\theta(\\dot x) \\right) \\text{ is small.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7fe553",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We approximate this via the % of misclassifications on $\\dot{\\mathcal{D}}$ sampled from $\\mathbb{P}$:\n",
    "$$\n",
    "\\frac{1}{m} \\sum_{i=1}^m \\mathbb{I}\\left(\\dot y^{(i)} \\neq f_\\theta(\\dot x^{(i)}) \\right) \\text{ is small.}\n",
    "$$\n",
    "\n",
    "Here, $\\mathbb{I}\\left( A \\right)$ is an *indicator* function that equals one if $A$ is true and zero otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0986ba",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To summarize, a supervised model $f_\\theta$ performs well when\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{(\\dot x, \\dot y)\\sim \\mathbb{P}} \\left[ L\\left(\\dot y, f_\\theta(\\dot x \\right)) \\right] \\text{ is \"good\"}.\n",
    "$$\n",
    "\n",
    "Under which conditions is supervised learning guaranteed to give us a good model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cef3c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning Provably Works\n",
    "\n",
    "Suppose that we choose $f \\in \\mathcal{M}$ on a dataset $\\mathcal{D}$ of size $n$ sampled IID from $\\mathbb{P}$ by minimizing\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^n L\\left(y^{(i)}, f(x^{(i)}) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02481b14",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let $f^*$, the best model in $\\mathcal{M}$:\n",
    "$$\n",
    "f^* = \\arg\\min_f \\mathbb{E}_{(\\dot x, \\dot y)\\sim \\mathbb{P}} \\left[ L\\left(\\dot y, f(\\dot x \\right)) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80fc60c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then, as $n \\to \\infty$, the performance of $f$ approaches that of $f^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f699581",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Datasets for Model Development\n",
    "\n",
    "When developing machine learning models, it is customary to work with three datasets: training -> selection -> evaluation\n",
    "* __Training set__: Data on which we train our algorithms.\n",
    "* __Development set__ (validation or holdout set): Data used for tuning algorithms.\n",
    "* __Test set__: Data used to evaluate the final performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc9f5b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Validation and Test Sets\n",
    "\n",
    "These holdout sets are used to esimate real-world performance. How should one choose the development and test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f8deb6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Distributional Consistency__: The development and test sets should be from the data distribution we will see in production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89418a3d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Dataset Size__: Should be large enough to estimate future performance: 30% of the data on small tasks, ususally up to not more than a few thousand instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e86e28f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cross-Validation\n",
    "\n",
    "If we don't have enough data for a validation set, we can do $K$-fold cross-validation.\n",
    "\n",
    "<center><img width=90% src=\"img/kfold_cv.gif\"></center>\n",
    "\n",
    "We group the data into $K$ disjoint folds. We train the model $K$ times, each time using a different fold for testing, and the rest for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3008c76e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Review: Polynomial Regression\n",
    "\n",
    "In 1D polynomial regression, we fit a model\n",
    "$$ f_\\theta(x) := \\theta^\\top \\phi(x) = \\sum_{j=0}^p \\theta_j x^j $$\n",
    "that is linear in $\\theta$ but non-linear in $x$ because the features \n",
    "$$\\phi(x) = [1\\; x\\; \\ldots\\; x^p]$$ \n",
    "are non-linear. Using these features, we can fit any polynomial of degree $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61158311",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Polynomials Fit the Data Well\n",
    "\n",
    "When we switch from linear models to polynomials, we can better fit the data and increase the accuracy of our models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb77cf7a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Towards Higher-Degree Polynomial Features?\n",
    "\n",
    "As we increase the complexity of our model class $\\mathcal{M}$ to include even higher degree polynomials, we are able to fit the data even better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51e7cc9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As the degree of the polynomial increases to the size of the dataset, we are increasingly able to fit every point in the dataset.\n",
    "\n",
    "However, this results in a highly irregular curve: its behavior outside the training set is wildly inaccurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34494ecb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overfitting\n",
    "\n",
    "Overfitting is one of the most common failure modes of machine learning.\n",
    "* A very expressive model (e.g., a high degree polynomial) fits the training dataset perfectly.\n",
    "* But the model makes highly incorrect predictions outside this dataset, and doesn't generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c07c43a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Underfitting\n",
    "\n",
    "A related failure mode is underfitting.\n",
    "\n",
    "* A small model (e.g. a straight line), will not fit the training data well.\n",
    "* Therefore, it will also not be accurate on a holdout set.\n",
    "\n",
    "<!-- __Underfitting__: On one hand, if the model is too small (like the linear model in the above example), it will not generalize well to unseen data because it is not sufficiently complex to fit the true structure of the dataset.\n",
    "\n",
    "__Overfitting__: On the other hand, if the model is too expressive (like a high degree polynomial), we are going to fit the training dataset perfectly; however, the model will make wildly incorrect prediction at points right outside this dataset, and will also not generalize well to unseen data. -->\n",
    "\n",
    "Finding the tradeoff between overfitting and underfitting is one of the main challenges in applying machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1f44ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overfitting vs. Underfitting: Evaluation\n",
    "\n",
    "We can measure overfitting and underfitting by estimating accuracy on held out data and comparing it to the training data.\n",
    "* If training perforance is high but holdout performance is low, we are overfitting.\n",
    "* If training perforance is low but holdout performance is low, we are underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bef6382",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to Fix Underfitting\n",
    "\n",
    "What if our model doesn't fit the training set well? Try the following:\n",
    "* Create richer features that will make the dataset easier to fit.\n",
    "* Use a more expressive model family (neural nets vs. linear models)\n",
    "* Try a better optimization procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe1db56",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to Fix Overfitting\n",
    "\n",
    "We will see many ways of dealing with overftting, but here are some ideas:\n",
    "* Use a simpler model family (linear models vs. neural nets)\n",
    "* Keep the same model, but collect more training data\n",
    "* Modify the training process to **penalize overly complex models.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586755b3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regularization: Intuition\n",
    "\n",
    "The idea of regularization is to penalize complex models that may overfit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f861ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regularization: Definition\n",
    "\n",
    "The idea of regularization is to train models with an augmented objective $J : \\mathcal{M} \\to \\mathbb{R}$ defined over a training dataset $\\mathcal{D}$ of size $n$ as\n",
    "\n",
    "$$J(f) = \\underbrace{\\frac{1}{n} \\sum_{i=1}^n L(y^{(i)}, f(x^{(i)}))}_\\text{Learning Objective} + \\underbrace{\\lambda \\cdot R(f)}_\\text{New Regularization Term}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869f3544",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's dissect the components of this objective:\n",
    "\n",
    "$$J(f) = \\frac{1}{n} \\sum_{i=1}^n L(y^{(i)}, f(x^{(i)})) + \\lambda \\cdot R(f).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4286d20c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* A loss function $L(y, f(x))$ such as the mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f59e33e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* A regularizer $R : \\mathcal{M} \\to \\mathbb{R}$ that penalizes models that are overly complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc604fe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* A regularization parameter $\\lambda > 0$, which controls the strength of the regularizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e320cfe",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When the model $f_\\theta$ is parametrized by parameters $\\theta$, we also use the following notation:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{n} \\sum_{i=1}^n L(y^{(i)}, f_\\theta(x^{(i)})) + \\lambda \\cdot R(\\theta).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c462dd9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Algorithm: Ridge Regression\n",
    "\n",
    "* __Type__: Supervised learning (regression)\n",
    "* __Model family__: Linear models\n",
    "* __Objective function__: L2-regularized mean squared error\n",
    "* __Optimizer__: Normal equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8b6650",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# L2 Regularization: Definition\n",
    "\n",
    "How can we define a regularizer $R: \\mathcal{M} \\to \\mathbb{R}$ to control the complexity of a model $f \\in \\mathcal{M}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4bc4e7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the context of linear models $f_\\theta(x) = \\theta^\\top x$, a widely used approach is L2 regularization, which defines the following objective:\n",
    "$$J(\\theta) = \\frac{1}{n} \\sum_{i=1}^n L(y^{(i)}, \\theta^\\top x^{(i)}) + \\frac{\\lambda}{2} \\cdot ||\\theta||_2^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681489eb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's dissect the components of this objective.\n",
    "$$J(\\theta) = \\frac{1}{n} \\sum_{i=1}^n L(y^{(i)}, \\theta^\\top x^{(i)}) + \\frac{\\lambda}{2} \\cdot ||\\theta||_2^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d840e9d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The regularizer $R : \\Theta \\to \\mathbb{R}$ is the function \n",
    "$R(\\theta) = ||\\theta||_2^2 = \\sum_{j=1}^d \\theta_j^2.$ \n",
    "This is also known as the L2 norm of $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cead06",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The regularizer penalizes large parameters. This prevents us from relying on any single feature and penalizes very irregular solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2033e9a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* L2 regularization can be used with most models (linear, neural, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee921df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# L2 Regularization for Polynomial Regression\n",
    "\n",
    "Let's consider an application to the polynomial model we have seen so far. Given polynomial features $\\phi(x)$, we optimize the following objective:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2n} \\sum_{i=1}^n \\left( y^{(i)} - \\theta^\\top \\phi(x^{(i)}) \\right)^2 + \\frac{\\lambda}{2} \\cdot ||\\theta||_2^2. $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c8b7b5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In order to define a very irregular function, we need very large polynomial weights.\n",
    "\n",
    "Forcing the model to use small weights prevents it from learning irregular functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616239c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Normal Equations for Regularized Models\n",
    "\n",
    "How, do we fit regularized models? As in the linear case, we can do this easily by deriving generalized normal equations! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa912fcd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let $L(\\theta) = \\frac{1}{2} (X \\theta - y)^\\top  (X \\theta - y)$ be our least squares objective. We can write the L2-regularized objective as:\n",
    "$$ J(\\theta) = \\frac{1}{2} (X \\theta - y)^\\top  (X \\theta - y) + \\frac{1}{2} \\lambda ||\\theta||_2^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf869044",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This allows us to derive the gradient as follows:\n",
    "\\begin{align*}\n",
    "\\nabla_\\theta J(\\theta) \n",
    "& = \\nabla_\\theta \\left( \\frac{1}{2} (X \\theta - y)^\\top  (X \\theta - y) + \\frac{1}{2} \\lambda ||\\theta||_2^2 \\right) \\\\\n",
    "& = \\nabla_\\theta \\left( L(\\theta) + \\frac{1}{2} \\lambda \\theta^\\top \\theta \\right) \\\\\n",
    "& = \\nabla_\\theta L(\\theta) + \\lambda \\theta \\\\\n",
    "& = (X^\\top X) \\theta - X^\\top y + \\lambda \\theta \\\\\n",
    "& = (X^\\top X + \\lambda I) \\theta - X^\\top y\n",
    "\\end{align*}\n",
    "\n",
    "We used the derivation of the normal equations for least squares to obtain $\\nabla_\\theta L(\\theta)$ as well as the fact that: $\\nabla_x x^\\top x = 2 x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f28604",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can set the gradient to zero to obtain normal equations for the Ridge model:\n",
    "$$ (X^\\top X + \\lambda I) \\theta = X^\\top y. $$\n",
    "\n",
    "Hence, the value $\\theta^*$ that minimizes this objective is given by:\n",
    "$$ \\theta^* = (X^\\top X + \\lambda I)^{-1} X^\\top y.$$\n",
    "\n",
    "Note that the matrix $(X^\\top X + \\lambda I)$ is always invertible, which addresses a problem with least squares that we saw earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c009310a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Algorithm: Lasso\n",
    "\n",
    "L1-regularized linear regression is also known as the Lasso (least absolute shrinkage and selection operator).\n",
    "\n",
    "* __Type__: Supervised learning (regression)\n",
    "* __Model family__: Linear models\n",
    "* __Objective function__: L1-regularized mean squared error\n",
    "* __Optimizer__: gradient descent, coordinate descent, least angle regression (LARS) and others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbaa6c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# L1 Regularizion: Definition\n",
    "\n",
    "Another closely related approach to regularization is to penalize the size of the weights using the L1 norm.\n",
    "\n",
    "In the context of linear models $f(x) = \\theta^\\top x$, L1 regularization yields the following objective:\n",
    "$$ J(\\theta) = \\frac{1}{n} \\sum_{i=1}^n L(y^{(i)}, \\theta^\\top x^{(i)}) + \\lambda \\cdot ||\\theta||_1. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec975ae5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's dissect the components of this objective.\n",
    "$$ J(\\theta) = \\frac{1}{n} \\sum_{i=1}^n L(y^{(i)}, \\theta^\\top x^{(i)}) + \\lambda \\cdot ||\\theta||_1. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b17008e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The regularizer $R : \\mathcal{M} \\to \\mathbb{R}$ is \n",
    "$R(\\theta) = ||\\theta||_1 = \\sum_{j=1}^d |\\theta_j|.$ \n",
    "This is known as the L1 norm of $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e53fdba",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* This regularizer also penalizes large weights. **It additionally forces most weights to decay to zero, as opposed to just being small.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f65622c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sparsity: Definition\n",
    "\n",
    "A vector is said to be sparse if a large fraction of its entires is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0368eab",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "L1-regularized linear regression produces *sparse parameters* $\\theta$.\n",
    "* This is makes the model more interpretable\n",
    "* It also makes it computationally more tractable in very large dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e3b14c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regularizing via Constraints\n",
    "\n",
    "Consider a regularized problem with a penalty term:\n",
    "$$ \\min_{\\theta \\in \\Theta} L(\\theta) + \\lambda \\cdot R(\\theta). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccb1f84",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Alternatively, we may enforce an explicit constraint on the complexity of the model:\n",
    "\\begin{align*}\n",
    "\\min_{\\theta \\in \\Theta} \\; & L(\\theta) \\\\\n",
    "\\text{such that } \\; & R(\\theta) \\leq \\lambda'\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e97386",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We will not prove this, but solving this problem is equivalent to solving the penalized problem for some $\\lambda > 0$ that's different from $\\lambda'$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cad643",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In other words, \n",
    "* We can regularize by explicitly enforcing $R(\\theta)$ to be less than a value instead of penalizing it.\n",
    "* For each value of $\\lambda$, we are implicitly setting a constraint of $R(\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0ad652",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# L1 vs. L2 Regularization\n",
    "\n",
    "The following image by <a href=\"https://medium.com/uwaterloo-voice/a-deep-dive-into-regularization-eec8ab648bce\">Divakar Kapil</a> and Hastie et al. explains the difference between the two norms.\n",
    "\n",
    "<left><img width=75% src=\"img/l1-vs-l2-annotated.png\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c381ec85",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sparsity: Ridge Model\n",
    "\n",
    "To better understand sparsity, we fit Ridge and Lasso on the UCI diabetes dataset and observe the magnitude of each weight (colored lines) as a function of the regularization parameter.\n",
    "\n",
    "Below is Ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a549550b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.466835921509635e-06,\n",
       " 223.872113856834,\n",
       " -868.4051623855127,\n",
       " 828.0533448059361)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# based on https://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_path.html\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import Ridge\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "\n",
    "# create ridge coefficients\n",
    "alphas = np.logspace(-5, 2,  )\n",
    "ridge_coefs = []\n",
    "for a in alphas:\n",
    "    ridge = Ridge(alpha=a, fit_intercept=False)\n",
    "    ridge.fit(X, y)\n",
    "    ridge_coefs.append(ridge.coef_)\n",
    "\n",
    "# plot ridge coefficients\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(alphas, ridge_coefs)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Regularization parameter (lambda)')\n",
    "plt.ylabel('Magnitude of model parameters')\n",
    "plt.title('Ridge coefficients as a function of the regularization')\n",
    "plt.axis('tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d728a0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sparsity: Ridge vs. Lasso\n",
    "\n",
    "The Ridge model does not produce sparse weights. Let's now compare it to a Lasso model.\n",
    "\n",
    "Observe how the Lasso parameters become progressively smaller, until they reach exactly zero, and then they stay at zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88dcc23a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-133.00520290292727, 3673.000247757282, -869.357335763701, 828.4524952229654)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Based on: https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_lars.html\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import lars_path\n",
    "\n",
    "# create lasso coefficients    \n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "_, _, lasso_coefs = lars_path(X, y, method='lasso')\n",
    "xx = np.sum(np.abs(lasso_coefs.T), axis=1)\n",
    "\n",
    "# plot ridge coefficients\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot('121')    \n",
    "plt.plot(alphas, ridge_coefs)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Regularization Strength (lambda)')\n",
    "plt.ylabel('Magnitude of model parameters')\n",
    "plt.title('Ridge coefficients as a function of regularization strength $\\lambda$')\n",
    "plt.axis('tight')\n",
    "\n",
    "# plot lasso coefficients\n",
    "plt.subplot('122') \n",
    "plt.plot(3500-xx, lasso_coefs.T)\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.ylabel('Magnitude of model parameters')\n",
    "plt.xlabel('Regularization Strength (lambda)')\n",
    "plt.title('LASSO coefficients as a function of regularization strength $\\lambda$')\n",
    "plt.axis('tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0e62b6",
   "metadata": {
    "colab_type": "text",
    "id": "i_f5u2x9nn6I",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<left><img width=25% src=\"img/cornell_tech2.svg\"></left>\n",
    "\n",
    "# Lecture 6: Generative Models and Naive Bayes\n",
    "\n",
    "### Applied Machine Learning\n",
    "\n",
    "__Volodymyr Kuleshov__<br>Cornell Tech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16157d67",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "An interesting instance of a classification problem is classifying text.\n",
    "* Includes a lot applied problems: spam filtering, fraud detection, medical record classification, etc.\n",
    "* Inputs $x$ are sequences of words of an arbitrary length.\n",
    "* The dimensionality of text inputs is usually very large, proportional to the size of the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ea8a09",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bag of Words Representations\n",
    "\n",
    "Perhaps the most widely used approach to representing text documents is called \"bag of words\".\n",
    "\n",
    "We start by defining a vocabulary $V$ containing all the possible words we are interested in, e.g.:\n",
    "$$ V = \\{\\text{church}, \\text{doctor}, \\text{fervently}, \\text{purple}, \\text{slow}, ...\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f575d6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A bag of words representation of a document $x$ is a function $\\phi(x) \\to \\{0,1\\}^{|V|}$ that outputs a feature vector\n",
    "$$\n",
    "\\phi(x) = \\left( \n",
    "\\begin{array}{c}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\begin{array}{l}\n",
    "\\;\\text{church} \\\\\n",
    "\\;\\text{doctor} \\\\\n",
    "\\;\\text{fervently} \\\\\n",
    "\\\\\n",
    "\\;\\text{purple} \\\\\n",
    "\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "of dimension $V$. The $j$-th component $\\phi(x)_j$ equals $1$ if $x$ convains the $j$-th word in $V$ and $0$ otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5fb039",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification Using BoW Features\n",
    "\n",
    "Let's now have a look at the performance of classification over bag of words features.\n",
    "\n",
    "Now that we have a feature representation $\\phi(x)$, we can apply the classifier of our choice, such as logistic regression.\n",
    "\n",
    "* Once text is featurized, any off-the-shelf supervised learning algorithm can be applied, but some work better than others, as we will see next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5444d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Review: Supervised Learning Models\n",
    "\n",
    "A supervised learning model is a function\n",
    "$$ f_\\theta : \\mathcal{X} \\to \\mathcal{Y} $$\n",
    "that maps inputs $x \\in \\mathcal{X}$ to targets $y \\in \\mathcal{Y}$.\n",
    "\n",
    "Models have *parameters* $\\theta \\in \\Theta$ living in a set $\\Theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7c933a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, **logistic regression is a binary classification algorithm** which uses a model \n",
    "$$f_\\theta : \\mathcal{X} \\to [0,1]$$ \n",
    "of the form\n",
    "$$ f_\\theta(x) = \\sigma(\\theta^\\top x) = \\frac{1}{1 + \\exp(-\\theta^\\top x)}, $$\n",
    "where $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$ is the *sigmoid* or *logistic* function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab69a8e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, our logistic model defines (\"parameterizes\") a probability distribution $P_\\theta(y|x) : \\mathcal{X} \\times \\mathcal{Y} \\to [0,1]$ as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "P_\\theta(y=1 | x) & = \\sigma(\\theta^\\top x) \\\\\n",
    "P_\\theta(y=0 | x) & = 1-\\sigma(\\theta^\\top x).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1993c2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the logistic regression example, we optimize the following objective defined over a binary classification dataset  $\\mathcal{D} = \\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\ldots, (x^{(n)}, y^{(n)})\\}$.\n",
    "\\begin{align*}\n",
    "\\ell(\\theta) & = \\frac{1}{n}\\sum_{i=1}^n \\log P_\\theta (y^{(i)} \\mid x^{(i)}) \\\\\n",
    "& = \\frac{1}{n}\\sum_{i=1}^n {y^{(i)}} \\cdot \\log \\sigma(\\theta^\\top x^{(i)}) + (1-y^{(i)}) \\cdot \\log (1-\\sigma(\\theta^\\top x^{(i)})).\n",
    "\\end{align*}\n",
    "\n",
    "This objective is also often called the log-loss, or cross-entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20508ab3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Discriminative Models\n",
    "\n",
    "Logistic regression is an example of a *discriminative* machine learning model because \n",
    "* It directly transforms $x$ into a score for each class $y$ (e.g., via the formula $y=\\sigma(\\theta^\\top x)$)\n",
    "* It can be interpreted as defining a *conditional* probability $P_\\theta(y|x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb1072",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generative Models\n",
    "\n",
    "Another approach to classification is to use *generative* models.\n",
    "\n",
    "* A generative approach first builds a model of $x$ for each class:\n",
    "$$ P_\\theta(x | y=k) \\; \\text{for each class $k$}.$$\n",
    "$P_\\theta(x | y=k)$ *scores* each $x$ according to how well it matches class $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21d1398",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* A class probability $P_\\theta(y=k)$ encoding our prior beliefs\n",
    "$$ P_\\theta(y=k) \\; \\text{for each class $k$}.$$\n",
    "These are often just the % of each class in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd60377",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the context of spam classification, we would fit two models on a corpus of emails $x$ with spam/non-spam labels $y$:\n",
    "\n",
    "\\begin{align*}\n",
    "P_\\theta(x|y=\\text{0}) && \\text{and} && P_\\theta(x|y=\\text{1})\n",
    "\\end{align*}\n",
    "\n",
    "as well as define priors $P_\\theta(y=\\text{0}), P_\\theta(y=\\text{1})$.\n",
    "\n",
    "$P_\\theta(x | y=1)$ *scores* each $x$ based on how much it looks like spam.\n",
    "\n",
    "$P_\\theta(x | y=0)$ *scores* each $x$ based on how much it looks like non-spam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ca3c1e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Predictions From Generative Models\n",
    "\n",
    "Given a new $x'$, we return the class that is the most likely to have generated it:\n",
    "\\begin{align*}\n",
    "\\arg \\max_k P_\\theta(y=k | x') & = \\arg \\max_k  \\frac{P_\\theta(x' | y=k) P_\\theta(y=k)}{P_\\theta(x')} \\\\\n",
    "& = \\arg \\max_k P_\\theta(x' | y=k) P_\\theta(y=k),\n",
    "\\end{align*}\n",
    "where we have applied Bayes' rule in the first line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6817ce68",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the context of spam classification, given a new $x'$, we would compare the probabilities of both models:\n",
    "\n",
    "\\begin{align*}\n",
    "P_\\theta(x'|y=\\text{0})P_\\theta(y=\\text{0}) && \\text{vs.} && P_\\theta(x'|y=\\text{1})P_\\theta(y=\\text{1})\n",
    "\\end{align*}\n",
    "\n",
    "We output the class that's more likely to have generated $x'$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1482e8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probabilistic Interpretations\n",
    "\n",
    "A *generative* model defines $P_\\theta(x|y)$ and $P_\\theta(y)$, thus it also defines a distribution of the form $P_\\theta(x,y)$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\underbrace{P_\\theta(x,y) : \\mathcal{X} \\times \\mathcal{Y} \\to [0,1]}_\\text{generative model} & \\;\\; & \\underbrace{P_\\theta(y|x) : \\mathcal{X} \\times \\mathcal{Y} \\to [0,1]}_\\text{discriminative model}\n",
    "\\end{align*}\n",
    "\n",
    "**Discriminative models don't define any probability over the $x$'s. Generative models do.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522872f7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can learn a generative model $P_\\theta(x, y)$ by maximizing the *likelihood*:\n",
    "\n",
    "$$ \\max_\\theta \\frac{1}{n}\\sum_{i=1}^n \\log P_\\theta({x}^{(i)}, y^{(i)}). $$\n",
    "\n",
    "This says that we should choose parameters $\\theta$ such that the model $P_\\theta$ assigns a high probability to each training example $(x^{(i)}, y^{(i)})$ in the dataset $\\mathcal{D}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e477efdb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generative vs. Discriminative Approaches\n",
    "\n",
    "What are the pros and cons of generative and discirminative methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852f9d29",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* If we only care about prediction, we don't need a model of $P(x)$. It's simpler to only model $P(y|x)$ (what we care about).\n",
    "    * In practice, discriminative models are often be more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dce0fbf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* If we care about other tasks (generation, dealing with missing values, etc.) or if we know the true model is generative, we want to use the generative approach.\n",
    "\n",
    "More on this later!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475ad504",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A Generative Model for Text Classification\n",
    "\n",
    "In binary text classification, we fit two models on a labeled corpus:\n",
    "\\begin{align*}\n",
    "P_\\theta(x|y=\\text{0}) && \\text{and} && P_\\theta(x|y=\\text{1})\n",
    "\\end{align*}\n",
    "We also define priors $P_\\theta(y=\\text{0}), P_\\theta(y=\\text{1})$.\n",
    "\n",
    "Each model $P_\\theta(x | y=k)$ *scores* $x$ based on how much it looks like class $k$.\n",
    "The documents $x$ are in __bag-of-words__ representation.\n",
    "\n",
    "How do we choose $P_\\theta(x|y=k)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241acd8f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Review: Categorical Distribution\n",
    "\n",
    "A [Categorical](https://en.wikipedia.org/wiki/Categorical_distribution) distribution with parameters $\\theta$ is a probability\n",
    "over $K$ discrete outcomes $x \\in \\{1,2,...,K\\}$:\n",
    "\n",
    "$$\n",
    "P_\\theta(x = j) = \\theta_j.\n",
    "$$\n",
    "\n",
    "When $K=2$ this is called the [Bernoulli](https://en.wikipedia.org/wiki/Bernoulli_distribution)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f14cb7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# First Attempt at a Generative Model\n",
    "\n",
    "Note there is a finite number of $x$'s: each is a binary vector of size $d$.\n",
    "\n",
    "A first solution is to assume that $P(x|y=k)$ is a categorical distribution that assigns a probability to each possible state of $x$:\n",
    "$$\n",
    "P(x|y=k) = P_k \\left( \n",
    "\\begin{array}{c}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0 \n",
    "\\end{array}\n",
    "\\right.\n",
    "\\left.\n",
    "\\begin{array}{l}\n",
    "\\;\\text{church} \\\\\n",
    "\\;\\text{doctor} \\\\\n",
    "\\;\\text{fervently} \\\\\n",
    "\\vdots \\\\\n",
    "\\;\\text{purple}\n",
    "\\end{array}\n",
    "\\right) = \\theta_{xk} = 0.0012\n",
    "$$\n",
    "The $\\theta_{xk}$ is the probability of $x$ under class $k$. We want to learn these."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597285e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Problem: High Dimensionality\n",
    "\n",
    "How many parameters does a Categorical model $P(x|y=k)$ have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9135d2be",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* If the dimensionality $d$ of $x$ is high (e.g., vocabulary has size 10,000), $x$ can take a huge number of values ($2^{10000}$ in our example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532b3f3c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We need to specify $2^{d}-1$ parameters for the Categorical distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378bed67",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For comparison, there are $\\approx 10^{82}$ atoms in the universe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78519484",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Naive Bayes: An Example\n",
    "\n",
    "To deal with high-dimensional $x$, we choose a simpler model for $P_\\theta(y|x)$:\n",
    "1. We define a (Bernoulli) model with one parameter $\\psi_{jk} \\in [0,1]$ for the occurrence of each word $j$ in class $k$:\n",
    "$$P_\\theta(x_j = 1 \\mid y=k) = \\psi_{jk}$$\n",
    "$\\psi_{jk}$ is the probability that a document of class $k$ contains word $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9613991d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. We define the model $P_\\theta(x|y=k)$ for documents $x$ as the product of the occurrence probabilities of each of its words $x_j$:\n",
    "$$ P_\\theta(x|y=k) = \\prod_{j=1}^d P_\\theta(x_j \\mid y=k) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06a18f2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How many parameters does this new model have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21d042c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We have a distribution $P_\\theta(x_j = 1 \\mid y=k)$ for each word $j$ and each distribution has one parameter $\\psi_{jk}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f579c22b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The distribution $P_\\theta(x|y=k) = \\prod_{j=1}^d P_\\theta(x_j \\mid y=k)$ is the product of $d$ such one-parameter distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e98540",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We have $K$ distributions of the form $P_\\theta(x|y=k)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f142781",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Thus, we only need $Kd$ parameters instead of $K(2^d-1)$!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb64a7d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Naive Bayes Assumption Machine Learning\n",
    "\n",
    "The Naive Bayes assumption is a __general technique__ that can be used with any $d$-dimensional $x$. \n",
    "* We simplify the model for $x$ as:\n",
    "$$ P(x|y) = \\prod_{j=1}^d P(x_j \\mid y) $$\n",
    "* We choose a simple distribution family for $P(x_j \\mid y)$.\n",
    "\n",
    "This typcally makes the number of parameters linear instead of exponential in $d$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b77994",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Is Naive Bayes a Good Assumption?\n",
    "\n",
    "Naive Bayes assumes that words are uncorrelated, but in reality they are.\n",
    "  * If spam email contains \"bank\", it probably contains \"account\"\n",
    "  \n",
    "As a result, the probabilities estimated by Naive Bayes can be over- under under-confident."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f45c0fb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In practice, however, Naive Bayes is a very useful assumption that gives very good classification accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d4b713",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Defining Prior Distributions for Our Model\n",
    "\n",
    "We still need to define the distribution $P_\\theta(y=k)$.\n",
    "* This encodes our prior belief about $y$ before we see $x$.\n",
    "* It can also be learned from data.\n",
    "\n",
    "Since we have a small number of classes $K$, we may use a Categorical distribution with parameters $\\vec\\phi = (\\phi_1,...,\\phi_K)$ and learn $\\vec\\phi$ from data:\n",
    "\n",
    "$$ P_\\theta(y=k) = \\phi_k.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad47165",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bernoulli Naive Bayes Model\n",
    "\n",
    "The *Bernoulli Naive Bayes* model $P_\\theta(x,y)$ is defined for *binary data* $x \\in \\{0,1\\}^d$ (e.g., bag-of-words documents)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b3b6b9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The $\\theta$ contains prior parameters $\\vec\\phi = (\\phi_1,...,\\phi_K)$ and $K$ sets of per-class parameters $\\psi_k = (\\psi_{1k},...,\\psi_{dk})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7ec95a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The probability of the data $x$ for each class equals\n",
    "$$P_\\theta(x|y=k) = \\prod_{j=1}^d P(x_j \\mid y=k),$$\n",
    "where each $P_\\theta(x_j \\mid y=k)$ is a $\\text{Bernoullli}(\\psi_{jk})$.\n",
    "\n",
    "The probability over $y$ is Categorical:\n",
    "$P_\\theta(y=k) = \\phi_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8744b2c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Formally, we have:\n",
    "\\begin{align*}\n",
    "P_\\theta(y) & = \\text{Categorical}(\\phi_1,\\phi_2,\\ldots,\\phi_K) \\\\\n",
    "P_\\theta(x_j=1|y=k) & = \\text{Bernoullli}(\\psi_{jk}) \\\\\n",
    "P_\\theta(x|y=k) & = \\prod_{j=1}^d P_\\theta(x_j|y=k)\n",
    "\\end{align*}\n",
    "The parameters of the model are $\\theta = (\\phi_1,...,\\phi_K, \\psi_{11}, ...,\\psi_{dK})$.\n",
    "There are exactly $K(d+1)$ parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20e18f4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Algorithm: Bernoulli Naive Bayes\n",
    "\n",
    "* __Type__: Supervised learning (multi-class classification)\n",
    "* __Model family__: Products of Bernoulli distributions, categorical priors\n",
    "* __Objective function__: Log-likelihood.\n",
    "* __Optimizer__: Closed form solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9934e94",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning a Bernoulli Naive Bayes Model\n",
    "\n",
    "Given a dataset $\\mathcal{D} = \\{(x^{(i)}, y^{(i)})\\mid i=1,2,\\ldots,n\\}$, we want to optimize the log-likelihood $\\ell(\\theta) = \\log L(\\theta)$:\n",
    "\\begin{align*}\n",
    "\\ell & = \\sum_{i=1}^n \\log P_\\theta(x^{(i)}, y^{(i)}) = \\sum_{i=1}^n \\sum_{j=1}^d \\log P_\\theta(x^{(i)}_j | y^{(i)}) + \\sum_{i=1}^n \\log P_\\theta(y^{(i)}) \\\\\n",
    "& = \\sum_{k=1}^K \\sum_{j=1}^d \\underbrace{\\sum_{i :y^{(i)} =k} \\log P(x^{(i)}_j | y^{(i)} ; \\psi_{jk})}_\\text{all the terms that involve $\\psi_{jk}$} + \\underbrace{\\sum_{i=1}^n \\log P(y^{(i)} ; \\vec \\phi)}_\\text{all the terms that involve $\\vec \\phi$}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d7c32b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In equality #2, we use Naive Bayes: $P_\\theta(x,y)=P_\\theta(y) \\prod_{i=1}^d P(x_j|y)$; in the third one, we change the order of summation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a9c446",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Each $\\psi_{jk}$ for $k=1,2,\\ldots,K$ is found in only the following terms:\n",
    "$$ \\max_{\\psi_{jk}} \\ell(\\theta) = \\max_{\\psi_{jk}} \\sum_{i :y^{(i)} =k} \\log P(x^{(i)}_j | y^{(i)} ; \\psi_{jk}). $$\n",
    "Thus, optimization over $\\psi_{jk}$ can be carried out independently of all the other parameters by just looking at these terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366ce5e4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Similarly, optimizing for $\\vec \\phi = (\\phi_1, \\phi_2, \\ldots, \\phi_K)$ only involves a few terms:\n",
    "$$ \\max_{\\vec \\phi} \\sum_{i=1}^n \\log P_\\theta(x^{(i)}, y^{(i)} ; \\theta) = \\max_{\\vec\\phi} \\sum_{i=1}^n  \\log P_\\theta(y^{(i)} ; \\vec \\phi). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dd693d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning the Parameters $\\phi$\n",
    "\n",
    "Let's first consider the optimization over $\\vec \\phi = (\\phi_1, \\phi_2, \\ldots, \\phi_K)$. \n",
    "$$ \\max_{\\vec \\phi} \\sum_{i=1}^n  \\log P_\\theta(y=y^{(i)} ; \\vec \\phi). $$\n",
    "* We have $n$ datapoints, each having one of $K$ classes\n",
    "* We want to learn the most likely class probabilities $\\phi_k$ that generated this data\n",
    "\n",
    "What is the maximum likelihood $\\phi$ in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d37e27",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Inuitively, the maximum likelihood class probabilities $\\phi$ should just be the class proportions that we see in the data. \n",
    "\n",
    "Let's calculate this formally. Our objective $J(\\vec \\phi)$ equals\n",
    "\\begin{align*}\n",
    "J(\\vec\\phi) & = \\sum_{i=1}^n  \\log P_\\theta(y^{(i)} ; \\vec \\phi) = \\sum_{i=1}^n  \\log \\left( \\frac{\\phi_{y^{(i)}}}{\\sum_{k=1}^K \\phi_k}\\right) \\\\\n",
    "& = \\sum_{i=1}^n \\log \\phi_{y^{(i)}} - n \\cdot \\log \\sum_{k=1}^K \\phi_k \\\\ \n",
    "& = \\sum_{k=1}^K \\sum_{i : y^{(i)} = k} \\log \\phi_k - n \\cdot \\log \\sum_{k=1}^K \\phi_k\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb7ea73",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Taking the derivative and setting it to zero, we obtain \n",
    "$$ \\frac{\\phi_k}{\\sum_l \\phi_l} = \\frac{n_k}{n}$$\n",
    "for each $k$, where $n_k = |\\{i : y^{(i)} = k\\}|$ is the number of training targets with class $k$.\n",
    "\n",
    "Thus, the optimal $\\phi_k$ is just the proportion of data points with class $k$ in the training set!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f835a90",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning the Parameters $\\psi_{jk}$\n",
    "\n",
    "Next, let's look at the maximum likelihood term\n",
    "$$ \\arg\\max_{\\psi_{jk}} \\sum_{i :y^{(i)} =k} \\log P(x^{(i)}_j | y^{(i)} ; \\psi_{jk}). $$\n",
    "over the word parameters $\\psi_{jk}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da0666",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Our dataset are all the inputs $x$ for which $y=k$.\n",
    "* We seek  the probability $\\psi_{jk}$ of a word $j$ being present in a $x$.\n",
    "\n",
    "What is the maximum likelihood $\\psi_{jk}$ in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cd1549",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Each $\\psi_{jk}$ is simply the proportion of documents in class $k$ that contain the word $j$.\n",
    "\n",
    "We can maximize the likelihood exactly like we did for $\\phi$ to obtain closed form solutions:\n",
    "\\begin{align*}\n",
    "\\psi_{jk} = \\frac{n_{jk}}{n_k}.\n",
    "\\end{align*}\n",
    "where $|\\{i : x^{(i)}_j = 1 \\text{ and } y^{(i)} = k\\}|$ is the number of $x^{(i)}$ with label $k$ and a positive occurrence of word $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61294f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Querying the Model\n",
    "\n",
    "How do we ask the model for predictions? As discussed earler, we can apply Bayes' rule:\n",
    "$$\\arg\\max_y P_\\theta(y|x) = \\arg\\max_y P_\\theta(x|y)P(y).$$\n",
    "Thus, we can estimate the probability of $x$ and under each $P_\\theta(x|y=k)P(y=k)$ and choose the class that explains the data best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e342de",
   "metadata": {},
   "source": [
    "# Advantages of Naive Bayes\n",
    "\n",
    "Naive Bayes is a very important model in machine learning.\n",
    "\n",
    "* Usually much easier to train: we have closed form solutions for the optimal parameters!\n",
    "* Can deal with missing values, noisy inputs, and more!\n",
    "\n",
    "On many classification tasks, Naive Bayes matches the state-of-the-art."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aec069b",
   "metadata": {},
   "source": [
    "# Downsides of Naive Bayes\n",
    "\n",
    "Fundamentally, the modeling assumptions of Naive Bayes are incorrect:\n",
    "* May generate over- or under-confident predictions\n",
    "* Lower performance when assumptions fail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872c1058",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Review: Normal (Gaussian) Distribution\n",
    "\n",
    "A [multivariate normal](https://en.wikipedia.org/wiki/Multivariate_normal_distribution) distribution $P_\\theta(x) : \\mathcal{X} \\to [0,1]$ with parameters $\\theta = (\\mu, \\Sigma)$\n",
    "is a probability over a $d$-dimensional $x \\in \\mathbb{R}^d$\n",
    "\n",
    "$$\n",
    "P_\\theta(x; \\mu, \\Sigma) = \\frac{1}{\\sqrt{(2 \\pi)^d | \\Sigma |}} \\exp\\left(-\\frac{1}{2} (x - \\mu)^\\top \\Sigma^{-1} (x-\\mu) \\right)\n",
    "$$\n",
    "\n",
    "In one dimension, this reduces to $\\frac{1}{\\sqrt{2 \\pi}\\sigma} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2} \\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677c8a8f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Algorithm: Gaussian Discriminant Analysis (GDA): https://zhuanlan.zhihu.com/p/38269530 \n",
    "\n",
    "* __Type__: Supervised learning (multi-class classification)\n",
    "* __Model family__: Mixtures of Gaussians.\n",
    "* __Objective function__: Log-likelihood.\n",
    "* __Optimizer__: Closed form solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb93f4f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's use maximum likelihood to fit the Guassian Discriminant model. Note that model parameterss $\\theta$ are the union of the parameters of each sub-model:\n",
    "$$\\theta = (\\mu_1, \\Sigma_1, \\phi_1, \\ldots, \\mu_K, \\Sigma_K, \\phi_K).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e43558a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Mathematically, the components of the model $P_\\theta(x,y)$ are as follows.\n",
    "\\begin{align*}\n",
    "P_\\theta(y) & = \\frac{\\prod_{k=1}^K \\phi_k^{\\mathbb{I}\\{y = y_k\\}}}{\\sum_{k=1}^k \\phi_k} \\\\\n",
    "P_\\theta(x|y=k) & = \\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}} \\exp(-\\frac{1}{2}(x-\\mu_k)^\\top\\Sigma_k^{-1}(x-\\mu_k))\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa533fba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimizing the Log Likelihood\n",
    "\n",
    "Given a dataset $\\mathcal{D} = \\{(x^{(i)}, y^{(i)})\\mid i=1,2,\\ldots,n\\}$, we want to optimize the log-likelihood $\\ell(\\theta)$:\n",
    "\\begin{align*}\n",
    "\\ell(\\theta) & = \\sum_{i=1}^n \\log P_\\theta(x^{(i)}, y^{(i)}) = \\sum_{i=1}^n \\log P_\\theta(x^{(i)} | y^{(i)}) + \\sum_{i=1}^n \\log P_\\theta(y^{(i)}) \\\\\n",
    "& = \\sum_{k=1}^K  \\underbrace{\\sum_{i : y^{(i)} = k} \\log P(x^{(i)} | y^{(i)} ; \\mu_k, \\Sigma_k)}_\\text{all the terms that involve $\\mu_k, \\Sigma_k$} + \\underbrace{\\sum_{i=1}^n \\log P(y^{(i)} ; \\vec \\phi)}_\\text{all the terms that involve $\\vec \\phi$}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add5e5cb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In equality #2, we use the fact that $P_\\theta(x,y)=P_\\theta(y) P_\\theta(x|y)$; in the third one, we change the order of summation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3295ec34",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Each $\\mu_k, \\Sigma_k$ for $k=1,2,\\ldots,K$ is found in only the following terms:\n",
    "\\begin{align*}\n",
    "\\max_{\\mu_k, \\Sigma_k} \\sum_{i=1}^n \\log P_\\theta(x^{(i)}, y^{(i)})\n",
    "& = \\max_{\\mu_k, \\Sigma_k} \\sum_{l=1}^K  \\sum_{i : y^{(i)} = l} \\log P_\\theta(x^{(i)} | y^{(i)} ; \\mu_l, \\Sigma_l) \\\\\n",
    "& = \\max_{\\mu_k, \\Sigma_k} \\sum_{i : y^{(i)} = k} \\log P_\\theta(x^{(i)} | y^{(i)} ; \\mu_k, \\Sigma_k).\n",
    "\\end{align*}\n",
    "Thus, optimization over $\\mu_k, \\Sigma_k$ can be carried out independently of all the other parameters by just looking at these terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c87046",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Similarly, optimizing for $\\vec \\phi = (\\phi_1, \\phi_2, \\ldots, \\phi_K)$ only involves a few terms:\n",
    "$$ \\max_{\\vec \\phi} \\sum_{i=1}^n \\log P_\\theta(x^{(i)}, y^{(i)} ; \\theta) = \\max_{\\vec\\phi} \\\n",
    "\\sum_{i=1}^n  \\log P_\\theta(y^{(i)} ; \\vec \\phi). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf75e0c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning the Parameters $\\phi$\n",
    "\n",
    "Let's first consider the optimization over $\\vec \\phi = (\\phi_1, \\phi_2, \\ldots, \\phi_K)$. \n",
    "$$ \\max_{\\vec \\phi} \\sum_{i=1}^n  \\log P_\\theta(y=y^{(i)} ; \\vec \\phi). $$\n",
    "* We have $n$ datapoints. Each point has a label $k\\in\\{1,2,...,K\\}$.\n",
    "* Our model is a categorical and assigns a probability $\\phi_k$ to each outcome $k\\in\\{1,2,...,K\\}$.\n",
    "* We want to infer $\\phi_k$ assuming our dataset is sampled from the model.\n",
    "\n",
    "What are the maximum likelihood $\\phi_k$ that are most likely to have generated our data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3c4117",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Inuitively, the maximum likelihood class probabilities $\\phi$ should just be the class proportions that we see in the data. \n",
    "\n",
    "Let's calculate this formally. Our objective $J(\\vec \\phi)$ equals\n",
    "\\begin{align*}\n",
    "J(\\vec\\phi) & = \\sum_{i=1}^n  \\log P_\\theta(y^{(i)} ; \\vec \\phi) \\\\\n",
    "& = \\sum_{i=1}^n \\log \\phi_{y^{(i)}} - n \\cdot \\log \\sum_{k=1}^K \\phi_k \\\\ \n",
    "& = \\sum_{k=1}^K \\sum_{i : y^{(i)} = k} \\log \\phi_k - n \\cdot \\log \\sum_{k=1}^K \\phi_k\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1f5d55",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Taking the derivative and setting it to zero, we obtain \n",
    "$$ \\frac{\\phi_k}{\\sum_l \\phi_l} = \\frac{n_k}{n}$$\n",
    "for each $k$, where $n_k = |\\{i : y^{(i)} = k\\}|$ is the number of training targets with class $k$.\n",
    "\n",
    "Thus, the optimal $\\phi_k$ is just the proportion of data points with class $k$ in the training set!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba09c8eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning the Parameters $\\mu_k, \\Sigma_k$\n",
    "\n",
    "Next, let's look at the maximum likelihood term\n",
    "$$\\max_{\\mu_k, \\Sigma_k} \\sum_{i : y^{(i)} = k} \\log \\mathcal{N}(x^{(i)} | \\mu_k, \\Sigma_k)$$\n",
    "over the Gaussian parameters $\\mu_k, \\Sigma_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73330355",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Our dataset are all the points $x$ for which $y=k$.\n",
    "* We want to learn the mean and variance $\\mu_k, \\Sigma_k$ of a normal distribution that generates this data.\n",
    "\n",
    "What is the maximum likelihood $\\mu_k, \\Sigma_k$ in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37416d93",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Computing the derivative and setting it to zero, we obtain closed form solutions:\n",
    "\\begin{align*}\n",
    "\\mu_k & = \\frac{\\sum_{i: y^{(i)} = k} x^{(i)}}{n_k} \\\\\n",
    "\\Sigma_k & = \\frac{\\sum_{i: y^{(i)} = k} (x^{(i)} - \\mu_k)(x^{(i)} - \\mu_k)^\\top}{n_k} \\\\\n",
    "\\end{align*}\n",
    "**These are just the empirical means and covariances of each class.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3d61e8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Querying the Model\n",
    "\n",
    "How do we ask the model for predictions? As discussed earler, we can apply Bayes' rule:\n",
    "$$\\arg\\max_y P_\\theta(y|x) = \\arg\\max_y P_\\theta(x|y)P(y).$$\n",
    "Thus, we can estimate the probability of $x$ and under each $P_\\theta(x|y=k)P(y=k)$ and choose the class that explains the data best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0991f770",
   "metadata": {},
   "source": [
    "# Special Cases of GDA\n",
    "\n",
    "Many important generative algorithms are special cases of Gaussian Discriminative Analysis\n",
    "* Linear discriminant analysis (LDA): all the covariance matrices $\\Sigma_k$ take the same value.\n",
    "* Gaussian Naive Bayes: all the covariance matrices $\\Sigma_k$ are diagonal.\n",
    "* Quadratic discriminant analysis (QDA): another term for GDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c796679f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Generative vs. Discriminative Approaches\n",
    "\n",
    "Pros of discriminative models:\n",
    "* Often more accurate because they make fewer modeling assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc838680",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Pros of generative models:\n",
    "* Can do more than just prediction: generation, fill-in missing features, etc.\n",
    "* Can include extra prior knowledge; if prior knowledge is correct, model will be more accurate.\n",
    "* Often have closed-form solutions, hence are faster to train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79641980",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Discriminant Analysis\n",
    "\n",
    "When the covariances $\\Sigma_k$ in GDA are equal, we have an algorithm called Linear Discriminant Analysis or LDA.\n",
    "\n",
    "The probability of the data $x$ for each class is a multivariate Gaussian with the same covariance $\\Sigma$.\n",
    "$$P_\\theta(x|y=k) = \\mathcal{N}(x ; \\mu_k, \\Sigma).$$\n",
    "\n",
    "The probability over $y$ is Categorical:\n",
    "$P_\\theta(y=k) = \\phi_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c55cec",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Linear Discriminant Analysis outputs decision boundaries that are linear, just like Logistic/Softmax Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1e10e7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Softmax or Logistic regression also produce linear boundaries. In fact, both types of algorithms make use of the same model class.\n",
    "\n",
    "What is their difference then?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7646b2e3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What Is the LDA Model Class?\n",
    "\n",
    "We can derive a formula for $P_\\theta(y|x)$ in a Bernoulli Naive Bayes or LDA model when $K=2$:\n",
    "$$ P_\\theta(y|x) = \\frac{P_\\theta(x|y)P_\\theta(y)}{\\sum_{y'\\in \\mathcal{Y}}P_\\theta(x|y')P_\\theta(y')} = \\frac{1}{1+\\exp(-\\gamma^\\top x)} $$\n",
    "for some set of parameters $\\gamma$ (whose expression can be derived from $\\theta$). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955f96ce",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is the same form as Logistic Regression! Does it mean that the two sets of algorithms are equivalent? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1b5926",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "No! They assume the same model class $\\mathcal{M}$, **they use a different objective $J$ to select a model in $\\mathcal{M}$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1643f261",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Discriminative Approaches\n",
    "\n",
    "Discriminative algorithms are deservingly very popular.\n",
    "* Most state-of-the-art algorithms for classification are discriminative (including neural nets, boosting, SVMs, etc.)\n",
    "* They are often more accurate because they make fewer modeling assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666d245e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Other Useful Features of Generative Models\n",
    "\n",
    "Generative models can also do things that discriminative models can't do.\n",
    "* __Generation__: we can sample $x \\sim p(x|y)$ to generate new data (images, audio).\n",
    "* __Missing value imputation__: if $x_j$ is missing, we infer it using $p(x|y)$.\n",
    "* __Outlier detection__: we may detect via $p(x')$ if $x'$ is an outlier.\n",
    "* __Scalability__: Simple formulas for maximum likelihood parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79efdb7d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Generative Approaches\n",
    "\n",
    "But generative algorithms also have many advantages:\n",
    "* Can do more than just prediction: generation, fill-in missing features, etc.\n",
    "* Can include extra prior knowledge; if prior knowledge is correct, model will be more accurate.\n",
    "* Often have closed-form solutions, hence are faster to train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30df2e2",
   "metadata": {
    "colab_type": "text",
    "id": "i_f5u2x9nn6I",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<left><img width=25% src=\"img/cornell_tech2.svg\"></left>\n",
    "\n",
    "# Lecture 8: Unsupervised Learning\n",
    "\n",
    "### Applied Machine Learning\n",
    "\n",
    "__Volodymyr Kuleshov__<br>Cornell Tech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7880c7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "**We have a dataset *without* labels.** Our goal is to learn something interesting about the structure of the data:\n",
    "* Clusters hidden in the dataset.\n",
    "* Outliers: particularly unusual and/or interesting datapoints.\n",
    "* Useful signal hidden in noise, e.g. human speech over a noisy phone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03186a0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Unsupervised Dataset: Notation\n",
    "\n",
    "We define  of size $n$ a dataset for unsupervised learning as\n",
    "$$\\mathcal{D} = \\{x^{(i)} \\mid i = 1,2,...,n\\}$$\n",
    "\n",
    "Each $x^{(i)} \\in \\mathbb{R}^d$ denotes an input, a vector of $d$ attributes or features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2db01e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model: Notation\n",
    "\n",
    "We'll say that a model is a function\n",
    "$$ f : \\mathcal{X} \\to \\mathcal{S} $$\n",
    "that maps inputs $x \\in \\mathcal{X}$ to some notion of structure $s \\in \\mathcal{S}$.\n",
    "\n",
    "Structure can have many definitions (clusters, low-dimensional representations, etc.), and we will see many examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e27fb4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Often, models have *parameters* $\\theta \\in \\Theta$ living in a set $\\Theta$. We will then write the model as\n",
    "$$ f_\\theta : \\mathcal{X} \\to \\mathcal{S} $$\n",
    "to denote that it's parametrized by $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5b5c65",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model Class: Notation\n",
    "\n",
    "Formally, the model class is a set \n",
    "$$\\mathcal{M} \\subseteq \\{f \\mid f : \\mathcal{X} \\to \\mathcal{S} \\}$$\n",
    "of possible models that map input features to structural elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0286a0c9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When the models $f_\\theta$ are paremetrized by *parameters* $\\theta \\in \\Theta$ living in some set $\\Theta$. Thus we can also write\n",
    "$$\\mathcal{M} = \\{f_\\theta \\mid f : \\mathcal{X} \\to \\mathcal{S}; \\; \\theta \\in \\Theta \\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073117e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Objective: Notation\n",
    "\n",
    "We again define an *objective function* (also called a *loss function*)\n",
    "$$J(f) : \\mathcal{M} \\to [0, \\infty), $$\n",
    "which describes the extent to which $f$ \"fits\" the data $\\mathcal{D} = \\{x^{(i)} \\mid i = 1,2,...,n\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba9eafa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When $f$ is parametrized by $\\theta \\in \\Theta$, the objective becomes a function $J(\\theta) : \\Theta \\to [0, \\infty).$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7612726",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimizer: Notation\n",
    "\n",
    "An optimizer finds a model $f \\in \\mathcal{M}$ with the smallest value of the objective $J$.\n",
    "\\begin{align*}\n",
    "\\min_{f \\in \\mathcal{M}} J(f)\n",
    "\\end{align*}\n",
    "\n",
    "Intuitively, this is the function that bests \"fits\" the data on the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f04e64",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When $f$ is parametrized by $\\theta \\in \\Theta$, the optimizer minimizes a function $J(\\theta)$ over all $\\theta \\in \\Theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5ceae2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The $K$-Means Model\n",
    "\n",
    "We can use this dataset as input to a popular unsupervised learning algorithm, __$K$-means__.\n",
    "\n",
    "* The algorithm seeks to find $K$ hidden clusters in the data.\n",
    "* Each cluster is characterized by its centroid (its mean).\n",
    "* The clusters reveal interesting structure in the data.\n",
    "\n",
    "The parameters $\\theta$ of the model are $K$ *centroids* $c_1, c_2, \\ldots c_K \\in \\mathcal{X}$. The class of $x$ is $k$ if $c_k$ is the closest centroid to $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec53d740",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can think of the model returned by $K$-Means as a function\n",
    "$$f_\\theta : \\mathcal{X} \\to \\mathcal{S}$$\n",
    "that assigns each input $x$ to a cluster $s \\in \\mathcal{S} = \\{1,2,\\ldots,K\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edab39da",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# K-Means at a High Level\n",
    "\n",
    "At a high level, $K$-means performs the following steps. \n",
    "\n",
    "Starting from random clusters, we repeat until convergence:\n",
    "1. Set each centroid to be the center of the its cluster.\n",
    "2. Update each cluster: assign each point to its closest centroid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290f39a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The $K$-Means Objective\n",
    "\n",
    "How do we determine whether $f_\\theta$ is a good clustering of the dataset $\\mathcal{D}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e7d63b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We seek centroids $c_k$ such that the distance between the points and their closest centroid is minimized:\n",
    "$$J(\\theta) = \\sum_{i=1}^n || x^{(i)} - \\text{centroid}(f_\\theta(x^{(i)})) ||,$$\n",
    "where $\\text{centroid}(k) = c_k$ denotes the centroid for cluster $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097e9796",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The $K$-Means Optimizer\n",
    "\n",
    "We can optimize this in a two stop process, starting with an initial random cluster assignment $f(x)$.\n",
    "\n",
    "Repeat until convergence:\n",
    "1. Set each $c_k$ to be the center of the its cluster $\\{x^{(i)} \\mid f(x^{(i)}) = k\\}$.\n",
    "2. Update clustering $f(x)$ such that $x^{(i)}$ is in the cluster of its closest centroid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abdfd76",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$K$-Means has a number of limitations:\n",
    "* Clustering can get stuck in local minima\n",
    "* Measuring clustering quality is hard and relies on heuristics\n",
    "* Cluster assignment is binary and doesn't estimate confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b636f47f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Review: Generalization\n",
    "\n",
    "In machine learning, __generalization__ is the property of predictive models to achieve good performance on new, heldout data that is distinct from the training set.\n",
    "\n",
    "How does generalization apply to unsupervised learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bf0b98",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generalization in Unsupervised Learning\n",
    "\n",
    "We can think of the data distribution as being the sum of two distinct components $\\mathbb{P} = F + E$\n",
    "1. A signal component $F$ (hidden clusters, speech, low-dimensional data space, etc.)\n",
    "2. A random noise component $E$\n",
    "\n",
    "A machine learning model generalizes if it fits the true signal $F$; it overfits if it learns the noise $E$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c688d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Underfitting in Unsupervised Learning\n",
    "\n",
    "Underfitting happens when we are not able to fully learn the signal hidden in the data.\n",
    "\n",
    "In the context of $K$-Means, this means not capturing all the clusters in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe9f03b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overfitting in Unsupervised Learning\n",
    "\n",
    "Overfitting happens when we fit the noise, but not the signal.\n",
    "\n",
    "In our example, this means fitting small, local noise clusters rather than the true global clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19938de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Elbow Method: https://www.biaodianfu.com/k-means-choose-k.html\n",
    "\n",
    "The Elbow method is a way of tuning hyper-parameters in unsupervised learning.\n",
    "* We plot the objective function as a function of the hyper-parameter $K$.\n",
    "* The \"elbow\" of the curve happens when its rate of decrease substantially slows down.\n",
    "* The \"elbow' is a good guess for the hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2efa22a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Detecting Overfitting and Underfitting\n",
    "\n",
    "In unsupervised learning, overfitting and underfitting are more difficult to quantify than in supervised learning.\n",
    "* Performance may depend on our intuition and require human evaluation\n",
    "* If we know the true labels, we can measure the accuracy of the clustering\n",
    "\n",
    "If our model is probabilistic, we can detect overfitting without labels by comparing the log-likelihood between the training set and a holdout set (next lecture!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5cd5d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reducing Overfitting\n",
    "\n",
    "There are multiple ways to control for overfitting:\n",
    "1. Reduce model complexity (e.g., reduce $K$ in $K$-Means)\n",
    "2. Penalize complexity in objective (e.g., penalize large $K$)\n",
    "3. **Use a probabilistic model and regularize it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d1ba06",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "The concept of generalization applies to both supervised and unsupervised learning.\n",
    "* In supervised learning, it is easier to quantify via the accuracy.\n",
    "* In unsupervised learning, we may not be able to easily detect overfitting, but it still happens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc211d1",
   "metadata": {
    "colab_type": "text",
    "id": "i_f5u2x9nn6I",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<left><img width=25% src=\"img/cornell_tech2.svg\"></left>\n",
    "\n",
    "# Lecture 9: Density Estimation\n",
    "\n",
    "### Applied Machine Learning\n",
    "\n",
    "__Volodymyr Kuleshov__<br>Cornell Tech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fdbb7b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why Use Probabilistic Models?\n",
    "\n",
    "There are many tasks that we can solve with a good model $P_\\theta$.\n",
    "1. Generation: sample new objects from $P_\\theta$, such as images.\n",
    "2. Structure learning: find interesting structure in $P_\\text{data}$\n",
    "3. Density estimation: approximate $P_\\theta \\approx P_\\text{data}$ and use it to solve any downstream task (generation, clustering, outlier detection, etc.).\n",
    "\n",
    "We are going to be interested in the latter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92036e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Density Estimation\n",
    "\n",
    "The problem of density estimation is to approximate the data distribution $P_\\text{data}$ with the model $P$.\n",
    "$$ P \\approx P_\\text{data}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f12ba8e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It's also a general learning task. We can solve many downstream tasks using a good model $P$:\n",
    "* Outlier and novelty detection\n",
    "* Generating new samples $x$\n",
    "* Visualizing and understanding the structure of $P_\\text{data}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cd51d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Histogram Density Estimation\n",
    "\n",
    "Perhaps the simplest approach to density estimation is by forming a histogram.\n",
    "\n",
    "A histogram partitions the input space $x$ into a $d$-dimensional grid and counts the number of points in each cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b64efb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Limitations of Histograms\n",
    "\n",
    "Histogram-based methods have a number of shortcomings.\n",
    "* The number of grid cells increases exponentially with dimension $d$.\n",
    "* The histogram is not \"smooth\".\n",
    "* The shape of the histogram depends on the bin positions.\n",
    "\n",
    "We will now try to address the last two limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75019674",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Algorithm: Kernel Density Estimation\n",
    "\n",
    "* __Type__: Unsupervised learning (density estimation).\n",
    "* __Model family__: Non-parametric. Sum of $n$ kernels.\n",
    "* __Objective function__: Log-likelihood to choose optimal bandwidth.\n",
    "* __Optimizer__: Grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f76542",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Kernel Density Estimation: Idea\n",
    "\n",
    "Kernel density estimation (KDE) is a different approach to histogram estimation.\n",
    "* A histogram has $b$ bins of width $\\delta$ at fixed positions.\n",
    "* KDE effectively places a bin of with $\\delta$ at each $x \\in \\mathcal{X}$.\n",
    "* To obtain $P(x)$, we count the % of points that fall in the bin centered at $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c1127a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tophat Kernel Density Estimation\n",
    "\n",
    "The simplest form of this strategy (Tophat KDE) assumes a model of the form\n",
    "$$P_\\delta(x) = \\frac{N(x; \\delta)}{n},$$\n",
    "where\n",
    "$$ N(x; \\delta) = |\\{x^{(i)} : ||x^{(i)} - x || \\leq \\delta/2\\}|, $$\n",
    "is the number of points that are within a bin of with $\\delta$ centered at $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1523eac",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The above algorithm still has the problem of producing a density estimate that is not smooth.\n",
    "\n",
    "We are going to resolve this by replacing histogram counts with weighted averages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9b8cae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Kernels\n",
    "\n",
    "A *kernel function* $K : \\mathcal{X} \\times \\mathcal{X} \\to [0, \\infty]$ maps pairs of vectors $x, z \\in \\mathcal{X}$ to a real-valued score $K(x,z)$.\n",
    "\n",
    "* A kernel represents the similarity between $x$ and $z$.\n",
    "* We will see many ways of defining \"similarity\"; they will all fit the framework that follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ddb451",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Kernel Density Estimation\n",
    "\n",
    "A kernelized density model $P$ takes the form:\n",
    "$$P(x) \\propto \\sum_{i=1}^n K(x, x^{(i)}).$$\n",
    "This can be interpreted in several ways:\n",
    "* We count the number of points \"near\" $x$, but each $x^{(i)}$ has a weight $K(x, x^{(i)})$ that depends on similarity between $x, x^{(i)}$.\n",
    "* We place a \"micro-density\" $K(x, x^{(i)})$ at each $x^{(i)}$; the final density $P(x)$ is their sum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04312fff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Types of Kernels\n",
    "\n",
    "We have seen several types of kernels in the context of support vector machines.\n",
    "\n",
    "There are additional kernels that are popular for density estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71504f4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The following kernels are available in `scikit-learn`.\n",
    "* Gaussian kernel $K(x,z; \\delta) \\propto \\exp(-||x-z||^2/2\\delta^2)$\n",
    "* Tophat kernel $K(x,z; \\delta) = 1 \\text{ if } ||x-z|| \\leq \\delta/2$ else $0$.\n",
    "* Epanechnikov kernel $K(x,z; \\delta) \\propto 1 - ||x-z||^2/\\delta^2$\n",
    "* Exponential kernel $K(x,z; \\delta) \\propto \\exp(-||x-z||/\\delta)$\n",
    "* Linear kernel $K(x,z; \\delta) \\propto (1 - ||x-z||/\\delta)^+$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a089648",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# KDE in Higher Dimensions\n",
    "\n",
    "In priciple, kernel density estimation also works in higher dimensions.\n",
    "\n",
    "However, the number of datapoints needed for a good fit incrases expoentially with the dimension, which limits the applications of this model in high dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad53e09",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Choosing Hyperparameters\n",
    "\n",
    "Each kernel has a notion of \"bandwidth\" $\\delta$. This is a hyperparameter that controls the \"smoothness\" of the fit.\n",
    "* We can choose it using inspection or heuristics like we did for $K$ in $K$-Means.\n",
    "* Because we have a probabilistic model, we can also estimate likelihood on a holdout dataset (more on this later!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3668df1a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pros and Cons of KDE\n",
    "\n",
    "Pros:\n",
    "* Can approximate any data distribution arbtrarily well.\n",
    "\n",
    "Cons:\n",
    "* Need to store entire dataset to make queries, which is computationally prohibitive.\n",
    "* Number of data needed scale exponentially with dimension (\"curse of dimensionality\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca4a6f1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Algorithm: K-Nearest Neighbors\n",
    "\n",
    "* __Type__: Supervised learning (regression and classification)\n",
    "* __Model family__: Consensus over $K$ training instances.\n",
    "* __Objective function__: Euclidean, Minkowski, Hamming, etc.\n",
    "* __Optimizer__: Non at training. Nearest neighbor search at inference using specialized search algorithms (Hashing, KD-trees).\n",
    "* __Probabilistic interpretation__: Directly approximating the density $P_\\text{data}(y|x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6accc866",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A Simple Classification Algorithm: Nearest Neighbors\n",
    "\n",
    "Suppose we are given a training dataset $\\mathcal{D} = \\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\ldots, (x^{(n)}, y^{(n)})\\}$. At inference time, we receive a query point $x'$ and we want to predict its label $y'$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8fbe7a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A really simple but suprisingly effective way of returning $y'$ is the *nearest neighbors* approach.\n",
    "* Given a query datapoint $x'$, find the training example $(x, y)$ in $\\mathcal{D}$ that's closest to $x'$, in the sense that $x$ is \"nearest\" to $x'$\n",
    "* Return $y$, the label of the \"nearest neighbor\" $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62181ece",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Choosing a Distance Function\n",
    "\n",
    "How do we select the point $x$ that is the closest to the query point $x'$? There are many options:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ef49d7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The Euclidean distance $|| x - x' ||_2 = \\sqrt{\\sum_{j=1}^d |x_j - x'_j|^2)}$ is a popular choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a603a8ff",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The Minkowski distance $|| x - x' ||_p = (\\sum_{j=1}^d |x_j - x'_j|^p)^{1/p}$ generalizes the Euclidean, L1 and other distances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bf7b9d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# K-Nearest Neighbors\n",
    "\n",
    "Intuitively, we expect the true decision boundary to be smooth. Therefore, we average $K$ nearest neighbors at a query point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8483a24d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Given a query datapoint $x'$, find the $K$ training examples $\\mathcal{N} = \\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\ldots, (x^{(K)}, y^{(K)})\\} \\subseteq D$ that are closest to $x'$.\n",
    "* Return $y_\\mathcal{N}$, the consensus label of the neighborhood $\\mathcal{N}$.\n",
    "\n",
    "The consesus $y_\\mathcal{N}$ can be determined by voting, weighted average, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db3fa23",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# KNN Estimates Data Distribution\n",
    "\n",
    "Suppose that the output $y'$ of KNN is the average target in the neighborhood $\\mathcal{N}(x')$ around the query $x'$.\n",
    "Observe that we can write:\n",
    "$$y' = \\frac{1}{K} \\sum_{(x, y) \\in \\mathcal{N}(x')} y \\approx \\mathbb{E}[y \\mid x'].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a92ad22",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* When $x \\approx x'$ and when $\\mathbb{P}$ is reasonably smooth, each $y$ for $(x,y) \\in \\mathcal{N}(x')$ is approximately a sample from $\\mathbb{P}(y\\mid x')$ (since $\\mathbb{P}$ doesn't change much around $x'$, $\\mathbb{P}(y\\mid x') \\approx \\mathbb{P}(y\\mid x)$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01045e96",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Thus $y'$ is essentially a Monte Carlo estimate of $\\mathbb{E}[y \\mid x']$ (the average of $K$ samples from $\\mathbb{P}(y\\mid x')$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e59dfec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Non-Parametric Models\n",
    "\n",
    "Nearest neighbors is an example of a *non-parametric* model. Parametric vs. non-parametric are is a key distinguishing characteristic for machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df5343d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A parametric model $f_\\theta(x) : \\mathcal{X} \\times \\Theta \\to \\mathcal{Y}$ is defined by a finite set of parameters $\\theta \\in \\Theta$ whose dimensionality is constant with respect to the dataset. Linear models of the form\n",
    "$$ f_\\theta(x) = \\theta^\\top x $$\n",
    "are an example of a parametric model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d58cc40",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In a non-parametric model, the function $f$ uses the entire training dataset (or a post-proccessed version of it) to make predictions, as in $K$-Nearest Neighbors. \n",
    "\n",
    "In other words, the complexity of the model increases with dataset size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52021394",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Non-parametric models have the advantage of not loosing any information at training time.\n",
    "\n",
    "However, they are also computationally less tractable and may easily overfit the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cf4ab6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pros and Cons of KNN\n",
    "\n",
    "Pros:\n",
    "* Can approximate any data distribution arbtrarily well.\n",
    "\n",
    "Cons:\n",
    "* Need to store entire dataset to make queries, which is computationally prohibitive.\n",
    "* Number of data needed scale exponentially with dimension (\"curse of dimensionality\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e5820e",
   "metadata": {
    "colab_type": "text",
    "id": "i_f5u2x9nn6I",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<left><img width=25% src=\"img/cornell_tech2.svg\"></left>\n",
    "\n",
    "# Lecture 10: Clustering\n",
    "\n",
    "### Applied Machine Learning\n",
    "\n",
    "__Volodymyr Kuleshov__<br>Cornell Tech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39e50aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gaussian Mixture Models\n",
    "\n",
    "Gaussian mixtures define a model of the form:\n",
    "$$P_\\theta (x,z) = P_\\theta (x | z) P_\\theta (z)$$\n",
    "* $z \\in \\mathcal{Z} = \\{1,2,\\ldots,K\\}$ is discrete and follows a categorical distribution $P_\\theta(z=k) = \\phi_k$.\n",
    "* $x \\in \\mathbb{R}$ is continuous; conditioned on $z=k$, it follows a Normal distribution $P_\\theta(x | z=k) = \\mathcal{N}(\\mu_k, \\Sigma_k)$.\n",
    "\n",
    "The parameters $\\theta$ are the $\\mu_k, \\Sigma_k, \\phi_k$ for all $k=1,2,\\ldots,K$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767813ac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gaussian Mixtures for Clustering\n",
    "\n",
    "GMMs are useful for supervised *and* unsupervised learning (clustering):\n",
    "$$P_\\theta (x,z) = P_\\theta (x | z) P_\\theta (z)$$\n",
    "* This model postulates that our observed data is comprised of  $K$ clusters with proportions specified by $\\phi_1,\\phi_2, \\ldots, \\phi_K$\n",
    "* The points within each cluster follow a Normal distribution\n",
    "* Each point belongs to the cluster most likely to have generated it (according to $P_\\theta(x|z=k)P_\\theta(z=k)$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847e2d66",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Intuitively, a GMM represents well the two clusters in the geyser dataset:\n",
    "\n",
    "Raw data | Single Gaussian | Mixture of Gaussians\n",
    "--|--|---\n",
    "<img width=90% src=\"img/oldfaithful_v2.png\"> | <img width=90% src=\"img/oldfSingle_v2.png\"> | <img width=90% src=\"img/oldfMOG_v2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f8b221",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Maximum Marginal Likelihood Learning\n",
    "\n",
    "Maximum marginal (log-)likelihood is a way of learning any proabilistic model on an unsupervised dataset $\\mathcal{D}$ by maximizing:\n",
    "$$\n",
    "\\frac{1}{n}\\sum_{i=1}^n \\log P_\\theta({x}^{(i)}) = \\frac{1}{n}\\sum_{i=1}^n \\log \\left(\\sum_{z \\in \\mathcal{Z}} P_\\theta({x}^{(i)}, z)\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ecce5d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* This asks $P_\\theta$ to assign a high probability to the training data in $\\mathcal{D}$. \n",
    "* However, we need to use $P(x) = \\sum_{z \\in \\mathcal{Z}} P(x,z)$ to compute this probability because $z$ is not observed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee838b00",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimizing Marginal Likelihood\n",
    "\n",
    "How do we optimize the marginal likelihood objective?\n",
    "$$\\max_\\theta \\frac{1}{n}\\sum_{i=1}^n \\log \\left(\\sum_{z \\in \\mathcal{Z}} P_\\theta({x}^{(i)}, z)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4738d81",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Note that we can't flip the sum and the log!\n",
    "* Because of that, our closed-form solutions don't apply\n",
    "* In fact, the objective now has many local minima!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2756506b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Optimizing the likelihood of mixture models is hard. \n",
    "\n",
    "A Gaussian has a single maximum, but a mixture has many and its objective is non-convex (hard to optimize).\n",
    "\n",
    "<center><img width=50% src=\"img/mogdensity1d_v2.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ed20c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recovering Clusters from GMMs\n",
    "\n",
    "Given a trained model $P_\\theta (x,z) = P_\\theta (x | z) P_\\theta (z)$, we can look at the *posterior* probability\n",
    "$$P_\\theta(z = k\\mid x) = \\frac{P_\\theta(z=k, x)}{P_\\theta(x)} = \\frac{P_\\theta(x | z=k) P_\\theta(z=k)}{\\sum_{l=1}^K P_\\theta(x | z=l) P_\\theta(z=l)}$$\n",
    "of a point $x$ belonging to class $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e63528e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The posterior defines a \"soft\" assignment of $x$ to each class.\n",
    "* This is in contrast to the hard assignments from $K$-Means."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83ea800",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Unlike in supervised learning, cluster assignments are latent.\n",
    "* Hence, there is not a closed form solution for $\\theta$. \n",
    "* We will see specialized algorithm for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe4330e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Expectation Maximization: Intuition\n",
    "\n",
    "Expecation maximization (EM) is an algorithm for maximizing marginal log-likelihood \n",
    "$$\\max_\\theta \\sum_{x^{(i)}\\in \\mathcal{D}} \\log \\left( \\sum_{z \\in \\mathcal{Z}}P_\\theta(x^{(i)}, z) \\right)$$\n",
    "that can also be used to learn Gaussian mixtures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb51a4b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We want to optimize the marginal log-likelihood\n",
    "$$\\max_\\theta \\sum_{x^{(i)}\\in \\mathcal{D}} \\log \\left( \\sum_{z \\in \\mathcal{Z}}P_\\theta(x^{(i)}, z) \\right).$$\n",
    "* If we know the true $z^{(i)}$ for each $x^{(i)}$, we maximize\n",
    "$$\\max_\\theta \\sum_{x^{(i)}, z^{(i)}\\in \\mathcal{D}} \\log \\left( P_\\theta(x^{(i)}, z^{(i)}) \\right).$$\n",
    "and it's easy to find the best $\\theta$ (use solution for supervised learning).\n",
    "* If we know $\\theta$, we can estimate the cluster assignments $z^{(i)}$ for each $i$ by computing $P_\\theta(z | x^{(i)})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d000f77",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Expectation maximization alternates between these two steps.\n",
    "\n",
    "1. (__E-Step__) Given an estimate $\\theta_t$ of the weights, compute $P_\\theta(z | x^{(i)})$.\n",
    "and use it to “hallucinate” expected cluster assignments $z^{(i)}$.\n",
    "2. (__M-Step__) Find a new $\\theta_{t+1}$ that maximizes the marginal log-likelihood by optimizing $P_\\theta(x^{(i)}, z^{(i)})$ given the $z^{(i)}$ from step 1.\n",
    "\n",
    "This process increases the marginal likelihood at each step and eventually converges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093dadd3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Expectation Maximization: Definition\n",
    "\n",
    "Formally, EM learns the parameters $\\theta$ of a latent-variable model $P_\\theta(x,z)$ over a dataset $\\mathcal{D} = \\{x^{(i)} \\mid i = 1,2,...,n\\}$ as follows.\n",
    "\n",
    "For $t=0,1,2,\\ldots$, repeat until convergence:\n",
    "1. (__E-Step__) For each $x^{(i)} \\in \\mathcal{D}$ compute $P_{\\theta_t}(z|x^{(i)})$\n",
    "2. (__M-Step__) Compute new weights $\\theta_{t+1}$ as\n",
    "\\begin{align*}\n",
    "\\theta_{t+1} & = \\arg\\max_{\\theta} \\sum_{i=1}^n \\mathbb{E}_{z^{(i)} \\sim P_{\\theta_t}(z|x^{(i)})} \\log P_{\\theta}(x^{(i)}, z^{(i)})\n",
    "\\end{align*}\n",
    "\n",
    "Since assignments $P_{\\theta_t}(z|x^{(i)})$ are \"soft\", M-step involves an expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b92e99a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Expectation Maximization: Definition\n",
    "\n",
    "Formally, EM learns the parameters $\\theta$ of a latent-variable model $P_\\theta(x,z)$ over a dataset $\\mathcal{D} = \\{x^{(i)} \\mid i = 1,2,...,n\\}$ as follows.\n",
    "\n",
    "For $t=0,1,2,\\ldots$, repeat until convergence:\n",
    "1. (__E-Step__) For each $x^{(i)} \\in \\mathcal{D}$ compute $P_{\\theta_t}(z|x^{(i)})$\n",
    "2. (__M-Step__) Compute new weights $\\theta_{t+1}$ as\n",
    "\\begin{align*}\n",
    "\\theta_{t+1} & = \\arg\\max_{\\theta} \\sum_{i=1}^n \\mathbb{E}_{z^{(i)} \\sim P_{\\theta_t}(z|x^{(i)})} \\log P_{\\theta}(x^{(i)}, z^{(i)}) \\\\\n",
    "& = \\arg\\max_{\\theta} \\sum_{i=1}^n \\sum_{k=1}^K P_{\\theta_t}(z=k|x^{(i)}) \\log P_{\\theta}(x^{(i)}, z=k)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003ee9de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Understanding the E-Step\n",
    "\n",
    "Intuitively, we hallucinate $z^{(i)}$ in the E-Step. \n",
    "\n",
    "In practice, the $P_{\\theta_t}(z|x^{(i)})$ define \"soft\" assignments, and we compute a vector of class probabilities for each $x^{(i)}$.\n",
    "<!-- * The $P_{\\theta_t}(z|x^{(i)})$ define \"soft\" assignments, and we compute a vector of class probabilities for each $x^{(i)}$.\n",
    "* We compute an expected values over $z^{(i)}$ instead of hallucinating one value. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c9643d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Understanding the M-Step\n",
    "\n",
    "Since class assignments from E-step are probabilistic, we maximize an expectation:\n",
    "\\begin{align*}\n",
    "\\theta_{t+1} & = \\arg\\max_{\\theta} \\sum_{i=1}^n \\mathbb{E}_{z^{(i)} \\sim P_{\\theta_t}(z|x^{(i)})} \\log P_{\\theta}(x^{(i)}, z^{(i)}) \\\\\n",
    "& = \\arg\\max_{\\theta} \\sum_{i=1}^n \\sum_{k=1}^K P_{\\theta_t}(z=k|x^{(i)}) \\log P_{\\theta}(x^{(i)}, z=k)\n",
    "\\end{align*}\n",
    "For many interesting models, this is tractable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6c5759",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pros and Cons of EM\n",
    "\n",
    "EM is a very important optimization algorithm in machine learning.\n",
    "* It is easy to implement and is guaranteed to converge.\n",
    "* It works in a lot of imporant ML models.\n",
    "\n",
    "Its limitations include:\n",
    "* It can get stuck in local optima.\n",
    "* We may not be able to compute $P_{\\theta_t}(z|x^{(i)})$ in every model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d03300d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deriving the E-Step\n",
    "\n",
    "In the E-step, we compute the posterior for each data point $x$ as follows\n",
    " $$P_\\theta(z = k\\mid x) = \\frac{P_\\theta(z=k, x)}{P_\\theta(x)} = \\frac{P_\\theta(x | z=k) P_\\theta(z=k)}{\\sum_{l=1}^K P_\\theta(x | z=l) P_\\theta(z=l)}$$\n",
    "$P_\\theta(z\\mid x)$ defines a vector of probabilities that $x$ originates from component $k$ given the current set of parameters $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a333ec2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deriving the M-Step\n",
    "\n",
    "At the M-step, we optimize the expected log-likelihood of our model.\n",
    "\n",
    "\\begin{align*}\n",
    "&\\max_\\theta \\sum_{x \\in D} \\mathbb{E}_{z \\sim P_{\\theta_t}(z|x)} \\log P_\\theta(x,z) = \\\\\n",
    "& \\max_\\theta \\left( \\sum_{k=1}^K \\sum_{x \\in D} P_{\\theta_t}(z_k|x) \\log P_\\theta(x|z_k) + \\sum_{k=1}^K \\sum_{x \\in D} P_{\\theta_t}(z_k|x) \\log P_\\theta(z_k) \\right)\n",
    "\\end{align*}\n",
    "\n",
    "As in supervised learning, we can optimize the two terms above separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5cf665",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will start with $P_\\theta(x\\mid z=k) = \\mathcal{N}(x; \\mu_k, \\Sigma_k)$. We have to find $\\mu_k, \\Sigma_k$ that optimize\n",
    "$$\n",
    "\\max_\\theta \\sum_{x^{(i)} \\in D} P(z=k|x^{(i)}) \\log P_\\theta(x^{(i)}|z=k)\n",
    "$$\n",
    "Note that this corresponds to fitting a Gaussian to a dataset whose elements $x^{(i)}$ each have a weight $P(z=k|x^{(i)})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6d294d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similarly to how we did this the supervised regime, we compute the derivative, set it to zero, and obtain closed form solutions:\n",
    "\\begin{align*}\n",
    "\\mu_k & = \\frac{\\sum_{i=1}^n P(z=k|x^{(i)}) x^{(i)}}{n_k} \\\\\n",
    "\\Sigma_k & = \\frac{\\sum_{i=1}^n P(z=k|x^{(i)}) (x^{(i)} - \\mu_k)(x^{(i)} - \\mu_k)^\\top}{n_k} \\\\\n",
    "n_k & = \\sum_{i=1}^n P(z=k|x^{(i)}) \\\\\n",
    "\\end{align*}\n",
    "Intuitively, the optimal mean and covariance are the emprical mean and convaraince of the dataset $\\mathcal{D}$ when each element $x^{(i)}$ has a weight $P(z=k|x^{(i)})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de22153",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similarly, we can show that the class priors are\n",
    "\\begin{align*}\n",
    "\\phi_k & = \\frac{n_k}{n} \\\\\n",
    "n_k & = \\sum_{i=1}^n P(z=k|x^{(i)})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fa8ea5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# EM in Gaussian Mixture Models\n",
    "\n",
    "EM learns the parameters $\\theta$ of a Gaussian mixture model $P_\\theta(x,z)$ over a dataset $\\mathcal{D} = \\{x^{(i)} \\mid i = 1,2,...,n\\}$ as follows.\n",
    "\n",
    "For $t=0,1,2,\\ldots$, repeat until convergence:\n",
    "1. (__E-Step__) For each $x^{(i)} \\in \\mathcal{D}$ compute $P_{\\theta_t}(z|x^{(i)})$\n",
    "2. (__M-Step__) Compute parameters $\\mu_k, \\Sigma_k, \\phi_k$ using the above formulas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b434f64a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Measuring Generalization Using Log-Likelihood\n",
    "\n",
    "Probabilistic unsupervised models optimize an objective that can be used to detect overfitting and underfitting by comparing performance between training and holdout sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d79eaf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Below, we visualize the performance (measured via negative log-likelihood) on training and holdout sets as $K$ increases.\n",
    "* Generalization is important for supervised and unsupervised learning.\n",
    "* A probabilistic model can detect overfitting by comparing the likelihood of training data vs. that of holdout data.\n",
    "* We can reduce overfitting by making the model less expressive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9547503",
   "metadata": {
    "colab_type": "text",
    "id": "i_f5u2x9nn6I",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<left><img width=25% src=\"img/cornell_tech2.svg\"></left>\n",
    "\n",
    "# Lecture 11: Dimensionality Reduction\n",
    "\n",
    "### Applied Machine Learning\n",
    "\n",
    "__Volodymyr Kuleshov__<br>Cornell Tech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5c5e32",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dimensionality Reduction:  Examples\n",
    "\n",
    "Consider a dataset $\\mathcal{D} = \\{x^{(i)} \\mid i = 1,2,...,n\\}$ of motorcylces, characterized by a set of attributes.\n",
    "* Attributes include size, color, maximum speed, etc.\n",
    "* Suppose that two attributes are closely correlated: e.g., $x^{(i)}_j$ is the speed in `mph` and $x^{(i)}_k$ is the speed in `km/h`.\n",
    "* The real dimensionality of the data is $d-1$!\n",
    "\n",
    "We would like to automatically identify the right data dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef7ab7c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "More generally, a dimensionality reduction algorithm learns from data an unsupervised model\n",
    "$$f_\\theta : \\mathcal{X} \\to \\mathcal{Z},$$\n",
    "where $\\mathcal{Z}$ is a low-dimensional representation of the data.\n",
    "\n",
    "For each input $x^{(i)}$, $f_\\theta$ computes a low-dimensional representation $z^{(i)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8ccdcf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Dimensionality Reduction\n",
    "\n",
    "Suppose $\\mathcal{X} = \\mathbb{R}^d$ and $\\mathcal{Z} = \\mathbb{R}^p$ for some $p < d$. The transformation \n",
    "$$f_\\theta : \\mathcal{X} \\to \\mathcal{Z}$$\n",
    "is a linear function with parameters $\\theta = W \\in \\mathbb{R}^{d \\times p}$:\n",
    "$$ z = f_\\theta(x) = W^\\top \\cdot x. $$\n",
    "The latent dimension $z$ is obtained from $x$ via a matrix $W$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d493a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Algorithm: Principal Component Analysis\n",
    "\n",
    "* __Type__: Unsupervised learning (dimensionality reduction)\n",
    "* __Model family__: Linear projection $W^\\top z$ of low-dimensional $z$\n",
    "* __Objective function__: Reconstruction error or variance maximization\n",
    "* __Optimizer__: Matrix eigendecomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abaaf31",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Principal Components Model\n",
    "\n",
    "Principal component analysis (PCA) assumes that \n",
    "* Datapoints $x \\in \\mathbb{R}^{d}$ live close to a low-dimensional subspace $\\mathcal{Z} = \\mathbb{R}^p$ of dimension $p<d$\n",
    "* The subspace $\\mathcal{Z} = \\mathbb{R}^p$ is spanned by a set of orthonormal vectors $w^{(1)}, w^{(2)}, \\ldots, w^{(p)}$\n",
    "* The data $x$ are approximated by a linear combination $\\tilde x$ of the $w^{(k)}$\n",
    "$$ x \\approx \\tilde x = \\sum_{k=1}^p w^{(k)} z_k = W z $$\n",
    "for some $z \\in \\mathcal{X}$ that are the coordinates of $\\tilde x$ in the basis $W$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df16a96f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this example, the data lives in a lower-dimensional 2D plane within a 3D space (image [credit](https://doc.plob.org/machine_learning/14_Dimensionality_Reduction.html)).\n",
    "\n",
    "<center><img width=50% src=\"img/pca_example_plane.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74f93b3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The model for PCA is a function $f_\\theta$ of the form\n",
    "$$ z = f_\\theta(x) = W^\\top x, $$\n",
    "where $\\theta = W$ and $W$ is a $d \\times p$ matrix of $p$ orthonormal column vectors denoted as $w^{(1)}, w^{(2)}, \\ldots, w^{(p)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1c3db5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This model enables performing two tasks:\n",
    "* __Encoding__: $z = W^\\top x$, finding the low-dimensional form of input $x$\n",
    "* __Decoding__: $\\tilde x = W  z$, converting a low-dimensional $z$ to a high-dimensional representation $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624daa30",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PCA Objective: Reconstruction\n",
    "\n",
    "How do we find a good subpace $\\mathcal{Z}$ as defined by a set of orthonormal vectors $W$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46860db8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A natural objective is to minimize the reconstruction error\n",
    "$$J_1(W) = \\sum_{i=1}^n \\| x^{(i)} - \\tilde x^{(i)} \\|_2^2 =\\sum_{i=1}^n \\| x^{(i)} - W W^\\top x^{(i)} \\|_2^2$$\n",
    "between each input $x^{(i)}$ and its approximate reconstruction $$\\tilde x^{(i)} = W \\cdot z^{(i)} = W\\cdot W^\\top \\cdot x^{(i)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d67a3c8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this example, if the points don't lie perfectly on a plane, we choose the plane such that the points' distance to it is minimized (image [credit](https://doc.plob.org/machine_learning/14_Dimensionality_Reduction.html)).\n",
    "\n",
    "<center><img width=80% src=\"img/pca_example.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14387a0b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PCA Objective: Maximizing Variance\n",
    "\n",
    "An alternative objective for learning a PCA model is maximizing variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056e6873",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How do we automatically identify such natural directions of variation in the data?\n",
    "One way to reduce the dimensionality of this dataset from is to project it along the following line.\n",
    "<center><img width=50% src=\"img/pca_projection1.png\"></center>\n",
    "Projected data is tightly clustered around its mean. It has *low variance*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696a8b35",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An alternative projection is along the following line. Data is much more spread out: it has *high variance* around its mean.\n",
    "<center><img width=50% src=\"img/pca_projection2.png\"></center>\n",
    "Our goal is to identify this direction automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f02d258",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We may formalize this as follows. \n",
    "* Let $\\hat{\\mathbb{E}}[f(x)]$ denote empirical expectation for any $f$:\n",
    "$$\\hat{\\mathbb{E}}[f(x)] = \\frac{1}{n}\\sum_{i=1}^n f(x^{(i)}). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e19e85a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Assume that we have centered the data, i.e.\n",
    "$$\\hat{\\mathbb{E}}[x] = 0 \\text{ and thus } \\hat{\\mathbb{E}}[W^\\top x] = W^\\top \\hat{\\mathbb{E}}[x] = 0.$$\n",
    "<!-- $$\\frac{1}{n}\\sum_{i=1}^n x^{(i)} = 0 \\text{ and thus } \\frac{1}{n}\\sum_{i=1}^n W^\\top x^{(i)} = 0.$$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5692b2ef",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The the variance of the projected data is\n",
    "\\begin{align*}\n",
    "\\hat{\\mathbb{E}}\\left[ \\| z - \\hat{\\mathbb{E}}[z] \\|^2 \\right] =\n",
    "\\hat{\\mathbb{E}}\\left[ \\| W^\\top x - \\hat{\\mathbb{E}}[W^\\top x] \\|^2 \\right] \n",
    "& = \\hat{\\mathbb{E}}\\left[ \\| W^\\top x \\|^2 \\right]\n",
    "\\end{align*}\n",
    "<!-- \\begin{align*}\n",
    "\\frac{1}{n} \\sum_{i=1}^n \\| W^\\top x^{(i)} - \\frac{1}{n}\\sum_{i=1}^n x^{(i)} \\|_2^2\n",
    "& = \\frac{1}{n} \\sum_{i=1}^n \\| W^\\top x^{(i)}\\|_2^2\n",
    "\\end{align*} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a6a2eb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus, the variance objective is simply\n",
    "$$J_2(W) = \\hat{\\mathbb{E}}\\left[ \\| W^\\top x \\|^2 \\right] = \\frac{1}{n} \\sum_{i=1}^n \\| W^\\top x^{(i)}\\|_2^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa16a161",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Equivalence Between PCA Objectives\n",
    "\n",
    "It turns out that minimizing reconstruction error and maximizing variance are equivalent.\n",
    "$$\\arg\\min_W J_1(W) = \\arg\\max_W J_2(W).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7576b124",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This image by [Alex Williams](http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/) provides intuition.\n",
    "\n",
    "<center><img width=80% src=\"img/pca_two_views.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da20c1d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pros and Cons of PCA\n",
    "\n",
    "PCA is perhaps the most widely used dimensionality reduction algorithm.\n",
    "* It is both highly intuitive and effective\n",
    "* It is also fast and easy to implement\n",
    "\n",
    "Its limitations include:\n",
    "* Linear projections may be too limited in some applications\n",
    "* Choosing the right dimension $p$ can be somewhat of an art"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
